<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-10-21 Mon 21:16 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>神经网络拟合“玩具数据”</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">神经网络拟合“玩具数据”</h1>
</header>
<div id="outline-container-orgbc73dff" class="outline-2">
<h2 id="orgbc73dff">预加载</h2>
<div class="outline-text-2" id="text-orgbc73dff">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np
<span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt

<span style="color: #6c71c4;">COLOR</span> = [<span style="color: #2aa198;">'tab:blue'</span>, <span style="color: #2aa198;">'tab:orange'</span>, <span style="color: #2aa198;">'tab:green'</span>, <span style="color: #2aa198;">'tab:red'</span>, <span style="color: #2aa198;">'tab:purple'</span>,
           <span style="color: #2aa198;">'tab:brown'</span>, <span style="color: #2aa198;">'tab:pink'</span>, <span style="color: #2aa198;">'tab:gray'</span>, <span style="color: #2aa198;">'tab:olive'</span>, <span style="color: #2aa198;">'tab:cyan'</span>]
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc485088" class="outline-2">
<h2 id="orgc485088">生成数据</h2>
<div class="outline-text-2" id="text-orgc485088">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">N</span> = 100
<span style="color: #6c71c4;">D</span> = 2
<span style="color: #6c71c4;">K</span> = 3
<span style="color: #6c71c4;">X</span> = np.zeros((N*K, D))
<span style="color: #6c71c4;">y</span> = np.zeros(N*K, dtype=<span style="color: #2aa198;">'uint8'</span>)
<span style="color: #6c71c4;">learning_rate</span> = 0.00001
<span style="color: #6c71c4;">reg_lambda</span> = 0.0001
<span style="color: #859900;">for</span> j <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(K):
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#30830;&#23450;&#27599;&#19968;&#20010;&#31867;&#30340;&#34892;&#33539;&#22260;</span>
    <span style="color: #6c71c4;">ix</span> = <span style="color: #268bd2;">range</span>(N*j, N*(j+1))
    <span style="color: #6c71c4;">r</span> = np.linspace(0.0, 1, N)
    <span style="color: #6c71c4;">t</span> = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2
    <span style="color: #405A61;"># </span><span style="color: #405A61;">C_&#23558;&#20004;&#20010;&#25968;&#32452;&#25353;&#21015;&#36827;&#34892;&#36830;&#25509;(&#21512;&#24182;)</span>
    <span style="color: #6c71c4;">X</span>[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
    <span style="color: #6c71c4;">y</span>[ix] = j


<span style="color: #6c71c4;">fig</span>, <span style="color: #6c71c4;">ax</span> = plt.subplots(figsize=(9.0, 6.0))
ax.scatter(X[:, 0], X[:, 1],
           c=<span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">map</span>(<span style="color: #859900;">lambda</span> x: COLOR[x], y)),
           s=40, cmap=plt.cm.Spectral)
plt.tight_layout(pad=0.0)
</pre>
</div>


<figure>
<img src="./images/example-for-nerual-network-945052.png" alt="example-for-nerual-network-945052.png">

</figure>
</div>
</div>
<div id="outline-container-orgbe7ba6b" class="outline-2">
<h2 id="orgbe7ba6b">初始化参数</h2>
<div class="outline-text-2" id="text-orgbe7ba6b">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">W</span> = 0.01 * np.random.randn(D, K)
<span style="color: #6c71c4;">B</span> = np.zeros((1, K))
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdc9b2d3" class="outline-2">
<h2 id="orgdc9b2d3">线性分类器计算得分</h2>
<div class="outline-text-2" id="text-orgdc9b2d3">
<p>
将所有输入乘以权重累加后，再加上偏差。计算出每一个样本，对应3个类的得分，直观上，
渴望正确的类获得更高的得分，换句话说，就是正确的类在三个类的占比应该最大。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">f_scores</span>(X, W, B):
    <span style="color: #859900;">return</span> np.dot(X, W) + B
</pre>
</div>
</div>
</div>

<div id="outline-container-org656f073" class="outline-2">
<h2 id="org656f073">计算损失</h2>
<div class="outline-text-2" id="text-org656f073">
<p>
使用交叉熵(softmax classifier)。\( L_i = -log(\frac{e^{f_{yi}}}{\sum_j e^{f_j}})
\)，假设以下情况，如果仅仅具有一个类时，那预测就是正确的类，那计算出来的损失应该
为0,而 log(1) = 0 这是取log的原因。在得分中，所在比例越少，说明损失越大，但从log
函数的性质看，在(0, 1)区间中，越接近0,越接近负无穷，所以取负号。
</p>

<p>
这个数据集的损失等于\(L = \frac{1}{N} \sum_i L_i + \frac{1}{2}\lambda \sum_k
\sum_l W_{k,i}^{2} \)，表示为样本的平均损失加上正规损失。
</p>

<p>
下面例子中，y代表着正确类别，同时也是下标。这里由于权重矩阵是随机生成的，所以预
测正确的概率应该为1/3,因此损失值大约为 -log(1/3) = 1.09，跟计算出来的一样。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">f_exp_scores</span>(scores, y):
    <span style="color: #6c71c4;">height</span> = scores.shape[0]
    <span style="color: #6c71c4;">exp_scores</span> = np.exp(scores)
    <span style="color: #6c71c4;">probs</span> = exp_scores / np.<span style="color: #268bd2;">sum</span>(exp_scores, axis=1, keepdims=<span style="color: #d33682;">True</span>)
    <span style="color: #6c71c4;">correct_logprobs</span> = -np.log(probs[<span style="color: #268bd2;">range</span>(height), y])
    <span style="color: #859900;">return</span> probs, correct_logprobs

<span style="color: #859900;">def</span> <span style="color: #268bd2;">f_loss</span>(correct_logprobs, W, reg_lambda):
    <span style="color: #6c71c4;">data_loss</span> = np.mean(correct_logprobs)
    <span style="color: #6c71c4;">regularztion_loss</span> = 1/2 * reg_lambda * np.<span style="color: #268bd2;">sum</span>(W*W)
    <span style="color: #6c71c4;">loss</span> = data_loss + regularztion_loss
    <span style="color: #859900;">return</span> loss
</pre>
</div>
</div>
</div>

<div id="outline-container-orgdf53e71" class="outline-2">
<h2 id="orgdf53e71">计算梯度</h2>
<div class="outline-text-2" id="text-orgdf53e71">
<p>
根据公式计算得到，可以直接得出损失函数的梯度。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">f_gradient</span>(X, W, probs, y, reg_lambda):
    <span style="color: #6c71c4;">dscores</span> = probs.copy()

    <span style="color: #6c71c4;">height</span> = probs.shape[0]
    <span style="color: #6c71c4;">dscores</span>[<span style="color: #268bd2;">range</span>(height), y] -= 1
    <span style="color: #6c71c4;">dscores</span> /= height

    <span style="color: #6c71c4;">dW</span> = np.dot(X.T, dscores) + reg_lambda * W
    <span style="color: #6c71c4;">dB</span> = np.<span style="color: #268bd2;">sum</span>(dscores, axis=0, keepdims=<span style="color: #d33682;">True</span>)
    <span style="color: #859900;">return</span> dW, dB
</pre>
</div>
</div>
</div>

<div id="outline-container-org9d677e5" class="outline-2">
<h2 id="org9d677e5">更新权重</h2>
<div class="outline-text-2" id="text-org9d677e5">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">f_update</span>(W, B, dW, dB, learning_rate):
    <span style="color: #6c71c4;">weights</span> = W.copy()
    <span style="color: #6c71c4;">bias</span> = B.copy()
    <span style="color: #6c71c4;">weights</span> -= learning_rate * dW
    <span style="color: #6c71c4;">bias</span> -= learning_rate * dB
    <span style="color: #859900;">return</span> weights, bias
</pre>
</div>
</div>
</div>

<div id="outline-container-org96a3685" class="outline-2">
<h2 id="org96a3685">迭代更新</h2>
<div class="outline-text-2" id="text-org96a3685">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">main</span>(X, W, B, y, reg_lambda, learning_rate, iter_num=100, verbose=<span style="color: #d33682;">False</span>):
    <span style="color: #859900;">for</span> i <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(iter_num):
        <span style="color: #6c71c4;">scores</span> = f_scores(X, W, B)
        <span style="color: #6c71c4;">probs</span>, <span style="color: #6c71c4;">correct_logprobs</span> = f_exp_scores(scores, y)
        <span style="color: #6c71c4;">loss</span> = f_loss(correct_logprobs, W, reg_lambda)
        <span style="color: #6c71c4;">dW</span>, <span style="color: #6c71c4;">dB</span> = f_gradient(X, W, probs, y, reg_lambda)
        <span style="color: #6c71c4;">W</span>, <span style="color: #6c71c4;">B</span> = f_update(W, B, dW, dB, learning_rate)
        <span style="color: #859900;">if</span> verbose:
            <span style="color: #859900;">print</span>(<span style="color: #2aa198;">"iter_num: %d, loss: %f"</span> %(i, loss))
    <span style="color: #859900;">return</span> W, B, loss

<span style="color: #6c71c4;">res_w</span>, <span style="color: #6c71c4;">res_b</span>, <span style="color: #6c71c4;">res_loss</span> = main(X, W, B, y, 1e-3, 1e-0, iter_num=200, verbose=<span style="color: #d33682;">True</span>)
</pre>
</div>
<p>
得到线性模型的损失函数为0.73多。
</p>
</div>
</div>

<div id="outline-container-org0db52b1" class="outline-2">
<h2 id="org0db52b1">神经网络</h2>
<div class="outline-text-2" id="text-org0db52b1">
<p>
从上面的线性分类器中，看到准确率仅仅51%。采用神经网络对数据进行拟合，设定一个两
层的网络，其中第一层网络具有100个节点，而第二层即最后一层具有3个(节点)分类。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">h</span> = 100
<span style="color: #6c71c4;">W</span> = 0.01 * np.random.randn(D, h)
<span style="color: #6c71c4;">B</span> = np.zeros((1, h))

<span style="color: #6c71c4;">W2</span> = 0.01 * np.random.randn(h, K)
<span style="color: #6c71c4;">B2</span> = np.zeros((1, K))

<span style="color: #6c71c4;">learning_rate</span> = 1e-0
<span style="color: #6c71c4;">reg_lambda</span> = 1e-3
</pre>
</div>
</div>

<div id="outline-container-org549d790" class="outline-3">
<h3 id="org549d790">计算得分</h3>
<div class="outline-text-3" id="text-org549d790">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">n_scores</span>(X, W, W2, B, B1):
    <span style="color: #6c71c4;">hidden_layer_scores</span> = np.dot(X, W) + B <span style="color: #405A61;"># </span><span style="color: #405A61;">(300, 100)</span>
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#28608;&#21169;&#20989;&#25968; ReLU</span>
    <span style="color: #6c71c4;">hidden_layer_scores</span> = np.maximum(0, hidden_layer_scores)

    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#36755;&#20986;&#23618;</span>
    <span style="color: #6c71c4;">output_scores</span> = np.dot(hidden_layer_scores, W2) + B2 <span style="color: #405A61;"># </span><span style="color: #405A61;">(300, 3)</span>
    <span style="color: #859900;">return</span> hidden_layer_scores, output_scores
</pre>
</div>
</div>
</div>
<div id="outline-container-org1b1c5c9" class="outline-3">
<h3 id="org1b1c5c9">计算损失</h3>
<div class="outline-text-3" id="text-org1b1c5c9">
<p>
损失函数同样是使用上面的softmax。根据反向传播算法。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">n_data_loss</span>(output_scores, y):
    <span style="color: #6c71c4;">height</span> = y.shape[0]
    <span style="color: #6c71c4;">exp_scores</span> = np.exp(output_scores)
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#24471;&#20998;&#22312;&#21508;&#31867;&#20013;&#30340;&#21344;&#27604;</span>
    <span style="color: #6c71c4;">exp_scores_percent</span> = exp_scores / np.<span style="color: #268bd2;">sum</span>(exp_scores, axis=1, keepdims=<span style="color: #d33682;">True</span>)
    <span style="color: #6c71c4;">corr_scores</span> = exp_scores_percent[<span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">range</span>(height)), y]
    <span style="color: #6c71c4;">data_loss</span> = np.mean(-np.log(corr_scores))
    <span style="color: #859900;">return</span> data_loss, exp_scores_percent

<span style="color: #859900;">def</span> <span style="color: #268bd2;">n_regularztion_loss</span>(W, W2, reg_lambda):
    <span style="color: #859900;">return</span> 0.5 * reg_lambda * (np.<span style="color: #268bd2;">sum</span>(W * W) + np.<span style="color: #268bd2;">sum</span>(W2 * W2))

<span style="color: #405A61;"># </span><span style="color: #405A61;">n_loss = n_data_loss(output_scores, y) + n_regularztion_loss(W, W2, reg_lambda)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgefef9e0" class="outline-3">
<h3 id="orgefef9e0">计算梯度</h3>
<div class="outline-text-3" id="text-orgefef9e0">
<p>
对损失函数求导。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">n_gradient</span>(X, W2, exp_scores_percent, y, hidden_layer_scores):
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23545;softmax&#20989;&#25968;&#27714;&#23548;&#37096;&#20998;&#65292;&#21069;&#38754;&#24050;&#32463;&#29992;&#20844;&#24335;&#35777;&#26126;&#65292;</span>
    <span style="color: #6c71c4;">height</span> = y.shape[0]
    <span style="color: #6c71c4;">doutput_scores</span> = exp_scores_percent.copy()
    <span style="color: #6c71c4;">doutput_scores</span>[<span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">range</span>(height)), y] -= 1
    <span style="color: #6c71c4;">doutput_scores</span> /= height

    <span style="color: #405A61;"># </span><span style="color: #405A61;">output_scores = np.dot(hidden_layer_scores, W2) + B2</span>
    <span style="color: #405A61;"># </span><span style="color: #405A61;">dw2 = hidden_layer_scores.T * doutput_scores</span>
    <span style="color: #6c71c4;">dW2</span> = np.dot(hidden_layer_scores.T, doutput_scores) <span style="color: #405A61;"># </span><span style="color: #405A61;">(100, 3)</span>
    <span style="color: #6c71c4;">dB2</span> = np.<span style="color: #268bd2;">sum</span>(doutput_scores, axis=0, keepdims=<span style="color: #d33682;">True</span>) <span style="color: #405A61;"># </span><span style="color: #405A61;">(1, 3)</span>

    <span style="color: #405A61;"># </span><span style="color: #405A61;">output_scores = np.dot(hidden_layer_scores, W2) + B2</span>
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#20808;&#35745;&#31639;output_scores&#23545;hidden_layer_scores&#30340;&#23548;&#25968;</span>
    <span style="color: #6c71c4;">dhidden_layer_scores</span> = np.dot(doutput_scores, W2.T) <span style="color: #405A61;"># </span><span style="color: #405A61;">(300, 100)</span>
    <span style="color: #405A61;"># </span><span style="color: #405A61;">Relu&#27714;&#23548;&#24471;&#65292;&#20165;&#20165;&#24403;x&#22823;&#20110;0&#65292;&#27714;&#23548;&#24471;1</span>
    <span style="color: #6c71c4;">dhidden_layer_scores</span>[hidden_layer_scores &lt;= 0] = 0

    <span style="color: #6c71c4;">dW</span> = np.dot(X.T, dhidden_layer_scores)
    <span style="color: #6c71c4;">dB</span> = np.<span style="color: #268bd2;">sum</span>(dhidden_layer_scores, axis=0, keepdims=<span style="color: #d33682;">True</span>)
    <span style="color: #859900;">return</span> dW, dB, dW2, dB2
</pre>
</div>
</div>
</div>

<div id="outline-container-orge345d0e" class="outline-3">
<h3 id="orge345d0e">更新函数</h3>
<div class="outline-text-3" id="text-orge345d0e">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">n_main</span>(X, y, h, W, B, W2, B2, learning_rate=1e-0, reg_lambda=1e-3, iter_num=500, verbose=<span style="color: #d33682;">False</span>):
    <span style="color: #6c71c4;">W</span> = W.copy()
    <span style="color: #6c71c4;">B</span> = B.copy()
    <span style="color: #6c71c4;">W2</span> = W2.copy()
    <span style="color: #6c71c4;">B2</span> = B2.copy()
    <span style="color: #859900;">for</span> i <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(iter_num):
        <span style="color: #6c71c4;">hidden_ls</span>, <span style="color: #6c71c4;">output_scores</span> = n_scores(X, W, W2, B, B2)
        <span style="color: #6c71c4;">data_loss</span>, <span style="color: #6c71c4;">exp_scores_percent</span> = n_data_loss(output_scores, y)
        <span style="color: #6c71c4;">reg_loss</span> = n_regularztion_loss(W, W2, reg_lambda)
        <span style="color: #6c71c4;">loss</span> = data_loss + reg_loss

        <span style="color: #6c71c4;">d_w</span>, <span style="color: #6c71c4;">d_b</span>, <span style="color: #6c71c4;">d_w2</span>, <span style="color: #6c71c4;">d_b2</span> = n_gradient(X, W2, exp_scores_percent, y, hidden_ls)
        <span style="color: #6c71c4;">d_w</span> += reg_lambda * W
        <span style="color: #6c71c4;">d_w2</span> += reg_lambda * W2

        <span style="color: #405A61;"># </span><span style="color: #405A61;">update</span>
        <span style="color: #6c71c4;">W</span>    -= learning_rate * d_w
        <span style="color: #6c71c4;">B</span>    -= learning_rate * d_b
        <span style="color: #6c71c4;">W2</span>   -= learning_rate * d_w2
        <span style="color: #6c71c4;">B2</span>   -= learning_rate * d_b2

        <span style="color: #859900;">if</span> verbose <span style="color: #859900;">and</span> i % 100 == 0:
            <span style="color: #859900;">print</span>(<span style="color: #2aa198;">"iter: %d, loss: %f"</span> %(i, loss))
    <span style="color: #859900;">return</span> W, B, W2, B2, loss

<span style="color: #6c71c4;">nres_W</span>, <span style="color: #6c71c4;">nres_B</span>, <span style="color: #6c71c4;">nres_W2</span>, <span style="color: #6c71c4;">nres_B2</span>, <span style="color: #6c71c4;">nres_loss</span> = n_main(X, y, h, W, B, W2, B2, iter_num=10000, verbose=<span style="color: #d33682;">True</span>)

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#35745;&#31639;&#20934;&#30830;&#29575;</span>
<span style="color: #6c71c4;">_</span>, <span style="color: #6c71c4;">n_s</span> = n_scores(X, nres_W, nres_W2, nres_B, nres_B2)
<span style="color: #6c71c4;">n_pred</span> = np.argmax(n_s, axis=1)
np.mean(n_pred == y)
</pre>
</div>
<p>
线性分类器，得到的损失函数值为0.78,而神经网络得到的损失值为0.24，神经网络的准确
率达到98%。
</p>
</div>
</div>
</div>

<div id="outline-container-org47b2eb9" class="outline-2">
<h2 id="org47b2eb9">决策边界</h2>
<div class="outline-text-2" id="text-org47b2eb9">
</div>
<div id="outline-container-org6cb0e77" class="outline-3">
<h3 id="org6cb0e77">线性函数决策边界</h3>
<div class="outline-text-3" id="text-org6cb0e77">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">step</span> = 0.02
<span style="color: #6c71c4;">xmin</span>, <span style="color: #6c71c4;">xmax</span> = X[:, 0].<span style="color: #268bd2;">min</span>() - 1, X[:, 0].<span style="color: #268bd2;">max</span>() + 1
<span style="color: #6c71c4;">ymin</span>, <span style="color: #6c71c4;">ymax</span> = X[:, 1].<span style="color: #268bd2;">min</span>() - 1, X[:, 1].<span style="color: #268bd2;">max</span>() + 1

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#20551;&#35774;  xx&#65292;yy&#37117;&#20026; (196, 191)</span>
<span style="color: #6c71c4;">xx</span>, <span style="color: #6c71c4;">yy</span> = np.meshgrid(np.arange(xmin, xmax, step),
                     np.arange(ymin, ymax, step))

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#23558;&#30697;&#38453;&#25289;&#24179;&#21518;&#65292;&#22312;&#21512;&#24182;&#25104;(196x191, 2)</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">(196x191, 2) &#20877;&#19982;W&#26435;&#37325;&#30697;&#38453;&#30456;&#20056;W(2, 3)&#65292;&#24471;&#21040;&#19968;&#20010;</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">(196x191, 3)&#20854;&#20013;&#27599;&#19968;&#34892;&#20195;&#34920;&#19968;&#20010;&#26679;&#26412;3&#20010;&#31867;&#21508;&#33258;&#30340;&#24471;&#20998;&#12290;</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">Z&#30456;&#31561;&#19982;&#21518;&#38754;&#30340;&#24471;&#20998;&#65292;&#21482;&#19981;&#36807;&#36825;&#37324;&#19981;&#26159;&#29992;X&#65292;&#32780;&#26159;&#29992;xx&#65292;yy</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#19978;&#38754;&#30340;&#35745;&#31639;&#24471;&#20998;&#30340;&#20989;&#25968;&#36827;&#34892;&#35745;&#31639;</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">Z = f_scores(np.c_[xx.ravel(), yy.ravel()], res_w, res_b)</span>
<span style="color: #6c71c4;">Z</span> = np.dot(np.c_[xx.ravel(), yy.ravel()], res_w) + res_b

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#20174;&#20013;&#36873;&#25321;&#26368;&#22823;&#30340;&#27010;&#29575;&#30340;&#31867;&#12290;</span>
<span style="color: #6c71c4;">Z</span> = np.argmax(Z, axis=1)        <span style="color: #405A61;"># </span><span style="color: #405A61;">(196x191, 1)</span>
<span style="color: #6c71c4;">Z</span> = Z.reshape(xx.shape)

<span style="color: #6c71c4;">fig</span> = plt.figure(figsize=(9.0, 6.0))
plt.contourf(xx, yy, Z, cmap=plt.cm.RdPu, alpha=0.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdPu, edgecolors=<span style="color: #2aa198;">'black'</span>)
plt.xlim(xx.<span style="color: #268bd2;">min</span>(), xx.<span style="color: #268bd2;">max</span>())
plt.ylim(yy.<span style="color: #268bd2;">min</span>(), yy.<span style="color: #268bd2;">max</span>())
plt.tight_layout(pad=0.0)
</pre>
</div>


<figure>
<img src="./images/example-for-nerual-network-480812.png" alt="example-for-nerual-network-480812.png">

</figure>
</div>
</div>
<div id="outline-container-orgd4ff3e2" class="outline-3">
<h3 id="orgd4ff3e2">神经网络决策边界</h3>
<div class="outline-text-3" id="text-orgd4ff3e2">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #405A61;"># </span><span style="color: #405A61;">&#35745;&#31639;&#22312;xx&#65292;yy&#19979;&#30340;&#24471;&#20998;</span>
<span style="color: #6c71c4;">_</span>, <span style="color: #6c71c4;">Z</span> = n_scores(np.c_[xx.ravel(), yy.ravel()], nres_W, nres_W2, nres_B, nres_B2)
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#20174;&#20013;&#36873;&#25321;&#26368;&#22823;&#30340;&#27010;&#29575;&#30340;&#31867;&#12290;</span>
<span style="color: #6c71c4;">Z</span> = np.argmax(Z, axis=1)
<span style="color: #6c71c4;">Z</span> = Z.reshape(xx.shape)

<span style="color: #6c71c4;">fig</span> = plt.figure(figsize=(9.0, 6.0))
plt.contourf(xx, yy, Z, cmap=plt.cm.RdPu, alpha=0.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdPu, edgecolors=<span style="color: #2aa198;">'black'</span>)
plt.xlim(xx.<span style="color: #268bd2;">min</span>(), xx.<span style="color: #268bd2;">max</span>())
plt.ylim(yy.<span style="color: #268bd2;">min</span>(), yy.<span style="color: #268bd2;">max</span>())
plt.tight_layout(pad=0.0)
</pre>
</div>


<figure>
<img src="./images/example-for-nerual-network-959414.png" alt="example-for-nerual-network-959414.png">

</figure>
</div>
</div>
</div>

<div id="outline-container-org17ed18a" class="outline-2">
<h2 id="org17ed18a">参考</h2>
<div class="outline-text-2" id="text-org17ed18a">
<p>
<a href="https://cs231n.github.io/neural-networks-case-study/">CS231n</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
