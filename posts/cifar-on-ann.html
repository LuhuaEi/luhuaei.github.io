<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-11-03 Sun 11:52 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>使用神经网络训练CIFAR(03)</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">使用神经网络训练CIFAR(03)</h1>
</header>
<div id="outline-container-org14e4035" class="outline-2">
<h2 id="org14e4035">激励函数</h2>
<div class="outline-text-2" id="text-org14e4035">
</div>
<div id="outline-container-org9129f88" class="outline-3">
<h3 id="org9129f88">为什么需要激励函数？</h3>
<div class="outline-text-3" id="text-org9129f88">
<p>
因为在现实中，有很多的数据都不是可以使用一个线或者一个平面进行分割的，而根据维基
百科的定义，线性函数是指：线性函数是只拥有一个参数的一阶多项式函数。所以需要引进
非线性函数用来解决线性函数的不足。这种位于层中的非线性函数被称为激励函数。
</p>

<p>
从另一个角度，如果没有激励函数，那该网络仅仅能表达线性处理，这样即使具有更多的隐
藏层，其功能都可以用单层的神经网络进行实现，表明，如果没有激励函数为模型提供非线
性转化，那隐藏层是没有作用的。
</p>

<p>
激励函数与损失函数：激励函数用于提供非线性功能，而损失函数用于计算预测结果与实际
结果之间的差异。
</p>

<p>
激励函数作为一个神经元，用于对输入变量进行处理，事实上任何数学函数都可以作为激励函数。
常见的激活函数有以下几个：
</p>
</div>
</div>
<div id="outline-container-orgdf07cc3" class="outline-3">
<h3 id="orgdf07cc3">Sigmoid</h3>
<div class="outline-text-3" id="text-orgdf07cc3">
<p>
非线性函数，取值范围为[0, 1], \(\sigma(x) = \frac{1}{1 + exp(-x)}\) 主要将一个值
归一化，即压缩到0与1之间，根据公式可以看出，大的负数值将会被求值为0，而大的正数
值将会被求值为1。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_sigmoid</span> = <span style="color: #859900;">lambda</span> x: 1.0 / (1.0 + np.exp(-x))
<span style="color: #859900;">def</span> <span style="color: #268bd2;">f_plot</span>(f, x):
    <span style="color: #6c71c4;">y</span> = <span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">map</span>(f, x))
    plt.style.use(<span style="color: #2aa198;">'ggplot'</span>)
    plt.plot(x, y, <span style="color: #2aa198;">'r-'</span>)
    plt.plot([0.0, 0.0], [<span style="color: #268bd2;">min</span>(y) -1, <span style="color: #268bd2;">max</span>(y)+1], <span style="color: #2aa198;">'b-'</span>)
    plt.plot([-10.0, 10.0], [0.0, 0.0], <span style="color: #2aa198;">'b-'</span>)
    plt.xlabel(<span style="color: #2aa198;">'x'</span>)
    plt.ylabel(<span style="color: #2aa198;">'y'</span>)

<span style="color: #6c71c4;">x</span> = np.arange(-10, 10, 0.001)
f_plot(f_sigmoid, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-393527.png" alt="cifar-on-ann-393527.png">

</figure>
</div>
<div id="outline-container-org46a2121" class="outline-4">
<h4 id="org46a2121">缺点：</h4>
<div class="outline-text-4" id="text-org46a2121">
<p>
在 sigmoid 的两端即位于0或者1上的点的梯度都是0(也称为：饱和)，如果一开始所给予过大
的权重，将会直接被判断为1，将不会被学习。函数输出的结果不是以0为中心化的，而且都
是为大于0。
</p>
</div>
</div>
</div>
<div id="outline-container-orgee228bf" class="outline-3">
<h3 id="orgee228bf">tanh(logistic)</h3>
<div class="outline-text-3" id="text-orgee228bf">
<p>
非线性函数，取值范围为[-1, 1]，Sigmod变体，但是为0中心化的，\(tanh(x) =
2\sigma(2x) - 1\)
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_tanh</span> = <span style="color: #859900;">lambda</span> x: 2.0 * f_sigmoid(2.0 * x) - 1.0
f_plot(f_tanh, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-891349.png" alt="cifar-on-ann-891349.png">

</figure>
</div>
</div>

<div id="outline-container-org088268b" class="outline-3">
<h3 id="org088268b">修正线性单元(ReLU)</h3>
<div class="outline-text-3" id="text-org088268b">
<p>
ReLU(Rectified Linear Unit)函数表示成\(ReLU(x) = max(0, x)\)。从斜率的角度看就是，
当x大于0,斜率为1,否则为0。因为ReLU的缺点，所以在使用时，需要小心模型学习速率，如
果很关心学习的速率，可以尝试Leaky ReLU或者Maxout。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_relu</span> = <span style="color: #859900;">lambda</span> x: x <span style="color: #859900;">if</span> x &gt; 0 <span style="color: #859900;">else</span> 0
f_plot(f_relu, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-723822.png" alt="cifar-on-ann-723822.png">

</figure>
</div>

<div id="outline-container-org3cce919" class="outline-4">
<h4 id="org3cce919">优点：</h4>
<div class="outline-text-4" id="text-org3cce919">
<p>
在随机梯度下降中，比tanh/Sigmod的收敛速度更快，并由于其是线性的，不会出现饱和现
象(指当达到一定的阀值后，值不会发生改变，如Sigmod中的，当达到一定程度后，很大的
数求值为1,很小的数求值为0，位于两点的梯度都为0，造成梯度消失(kill gradient)。与
tanh/Sigmod相比，计算量更小，只需要判断，而tanh/Sigmod需要计算指数。
</p>
</div>
</div>
<div id="outline-container-org93cc0a1" class="outline-4">
<h4 id="org93cc0a1">缺点：</h4>
<div class="outline-text-4" id="text-org93cc0a1">
<p>
ReLU很脆弱，如果当具有一个很大的梯度通过一个ReLU神经元时，将会导致这个神经元“死
掉”。这是因为大的梯度，导致权重更新的步伐过快，将可能导致可以通过调整更低的学习速率进行解决。
</p>
</div>
</div>
</div>
<div id="outline-container-org3cc8119" class="outline-3">
<h3 id="org3cc8119">泄漏ReLU(leaky ReLU)</h3>
<div class="outline-text-3" id="text-org3cc8119">
<p>
为了解决ReLU“死掉”问题而设计，当 x&lt;0 时，将给予一个微小的数，而不是0。就如，当
\(x<0 f(x) = ax\) 当 \(x>=0 f(x)=x\), 其中a代表一个常数，在一些方面可以取得很好
的结果，但一些方面结果并不令人满意。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_leaky_relu</span> = <span style="color: #859900;">lambda</span> x: x <span style="color: #859900;">if</span> x &gt;= 0 <span style="color: #859900;">else</span> 0.25*x
f_plot(f_leaky_relu, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-495479.png" alt="cifar-on-ann-495479.png">

</figure>
</div>
</div>

<div id="outline-container-org4d1f99b" class="outline-3">
<h3 id="org4d1f99b">Maxout</h3>
<div class="outline-text-3" id="text-org4d1f99b">
<p>
将多个激励函数进行合并，表达式为: \(max(F_1, F_2, ...)\) ，当后面的\(F_2, ...,
F_n\)为0时退化为ReLU函数，防止梯度消失(饱和)， 有避免ReLu的缺点(因为梯度过大导致
神经元“死掉”)，但这样，其对于每一个神经元必须需要传递两个参数以上的变量。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_maxout</span> = <span style="color: #859900;">lambda</span> x: f_sigmoid(x) <span style="color: #859900;">if</span> f_sigmoid(x) &gt; f_relu(x) <span style="color: #859900;">else</span> f_relu(x)
f_plot(f_maxout, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-890421.png" alt="cifar-on-ann-890421.png">

</figure>
</div>
</div>
</div>
<div id="outline-container-org6ce96fe" class="outline-2">
<h2 id="org6ce96fe">前向传播</h2>
<div class="outline-text-2" id="text-org6ce96fe">
<p>
前向传播就是平时看到的传播模式，就是一个一个接着往下传播。简单的前向传播实现，主
要用于理解前向传播的思想。神经网络中的 <code>channel</code> 是指每一个样本中的第三维度，就
图片来说，一般具有 <code>RGB</code> 三层。而 <code>depth</code> 是指隐藏层的个数，一般也称为内核个数。
</p>

<p>
添加 <code>zero padding</code> 是为了保持输出的采集结果维度与原来的相同。
</p>

<p>
采集结果的矩阵的维度计算：
</p>
<ul class="org-ul">
<li>\(height = (X_{height} - W_{height} + 2 * ZeroPadding) // stride + 1\)</li>
<li>\(wight = (X_{width} - W_{width} + 2 * ZeroPadding)\)</li>
</ul>

<p>
<code>np.pad()</code> 对一个矩阵进行填补。
</p>
</div>
<div id="outline-container-orgf156b52" class="outline-3">
<h3 id="orgf156b52">前向传播算法</h3>
<div class="outline-text-3" id="text-orgf156b52">
<p>
这个简单的算法使用的仅仅是线性组合。\(f(x_i, W, b) = W x_i + b\)
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">forward_pass</span>(input_volumes, input_weights, input_biases, stride=1, zero_padding=0):
    <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">    Inputs:</span>
<span style="color: #35a69c;">    ------------------------------------------------------------</span>
<span style="color: #35a69c;">    - input_volumns: the train dataset. (N, channel, width, height)</span>
<span style="color: #35a69c;">    - input_weights: the weights array. (K, channel, widht, height)</span>
<span style="color: #35a69c;">    - input_biase:   the biases. (K, )</span>
<span style="color: #35a69c;">    - zero_padding:  the size of padding for zero.</span>
<span style="color: #35a69c;">    Outputs:</span>
<span style="color: #35a69c;">    ------------------------------------------------------------</span>
<span style="color: #35a69c;">    - dout: The weights array collection result. (N, K, o_height, o_width)</span>
<span style="color: #35a69c;">    - cache: The parameters cache. (dict)</span>
<span style="color: #35a69c;">    '''</span>
    <span style="color: #6c71c4;">x_num</span>, <span style="color: #6c71c4;">x_channel</span>, <span style="color: #6c71c4;">x_height</span>, <span style="color: #6c71c4;">x_width</span> = input_volumes.shape
    <span style="color: #6c71c4;">w_num</span>, <span style="color: #6c71c4;">_</span>, <span style="color: #6c71c4;">w_height</span>, <span style="color: #6c71c4;">w_width</span> = input_weights.shape
    <span style="color: #6c71c4;">o_height</span> = (x_height - w_height + 2*zero_padding) // stride + 1
    <span style="color: #6c71c4;">o_width</span> = (x_width - w_width + 2*zero_padding) // stride + 1

    <span style="color: #6c71c4;">X</span> = np.pad(input_volumes, ((0, 0), (0, 0), (zero_padding, zero_padding), (zero_padding, zero_padding)),
               <span style="color: #2aa198;">"constant"</span>, constant_values=0)

    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#27599;&#19968;&#20010;&#37319;&#38598;&#22120;&#21482;&#20250;&#20135;&#29983;&#19968;&#20010;&#20108;&#32500;&#30340;&#25968;&#32452;</span>
    <span style="color: #6c71c4;">dout</span> = np.zeros((x_num, w_num, o_height, o_width))
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23545;&#26679;&#26412;&#36845;&#20195;&#65292;&#21363;&#23545;&#27599;&#19968;&#24352;&#22270;&#29255;&#36827;&#34892;&#36845;&#20195;</span>
    <span style="color: #859900;">for</span> n <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(x_num):
        <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23545;&#22810;&#20010;&#37319;&#38598;&#22120;&#36827;&#34892;&#36845;&#20195;&#37319;&#38598;</span>
        <span style="color: #859900;">for</span> k <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(w_num):
            <span style="color: #859900;">for</span> y <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(0, o_height):
                <span style="color: #859900;">for</span> x <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(0, o_width):
                    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#21033;&#29992;&#36755;&#20986;&#30340;&#32500;&#24230;&#65292;&#21453;&#25512;&#37319;&#38598;&#30340;&#21306;&#22495;</span>
                    <span style="color: #6c71c4;">dout</span>[n, k, y, x] = np.<span style="color: #268bd2;">sum</span>(
                        X[n, :, y*stride:y*stride + w_height, x*stride:x*stride + w_width] * input_weights[k]
                    ) + input_biases[k]
    <span style="color: #6c71c4;">cache</span> = (input_volumes, input_weights, input_biases, stride, zero_padding)
    <span style="color: #859900;">return</span> dout, cache
</pre>
</div>
</div>
</div>
<div id="outline-container-org0c648bf" class="outline-3">
<h3 id="org0c648bf">测试数据</h3>
<div class="outline-text-3" id="text-org0c648bf">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">x_shape</span> = (2, 3, 4, 4)
<span style="color: #6c71c4;">w_shape</span> = (3, 3, 4, 4)
<span style="color: #6c71c4;">x</span> = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)
<span style="color: #6c71c4;">w</span> = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)
<span style="color: #6c71c4;">b</span> = np.linspace(-0.1, 0.2, num=3)

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#27491;&#30830;&#30340;&#31572;&#26696;</span>
<span style="color: #6c71c4;">correct_out</span> = np.array([[[[-0.08759809, -0.10987781],
                          [-0.18387192, -0.2109216 ]],
                         [[ 0.21027089,  0.21661097],
                          [ 0.22847626,  0.23004637]],
                         [[ 0.50813986,  0.54309974],
                          [ 0.64082444,  0.67101435]]],
                        [[[-0.98053589, -1.03143541],
                          [-1.19128892, -1.24695841]],
                         [[ 0.69108355,  0.66880383],
                          [ 0.59480972,  0.56776003]],
                         [[ 2.36270298,  2.36904306],
                          [ 2.38090835,  2.38247847]]]])
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#35745;&#31639;&#30456;&#23545;&#35823;&#24046;</span>
<span style="color: #6c71c4;">dout</span>, <span style="color: #6c71c4;">_</span> = forward_pass(x, w, b, 2, 1)
<span style="color: #859900;">print</span>(<span style="color: #2aa198;">"relative_error: "</span>, relative_error(dout, correct_out))
</pre>
</div>

<pre class="example">
relative_error:  2.2121476417505994e-08

</pre>
</div>
</div>
</div>
<div id="outline-container-orgf1e9650" class="outline-2">
<h2 id="orgf1e9650">通过卷积处理图片</h2>
<div class="outline-text-2" id="text-orgf1e9650">
<p>
读取图片，在原来的 <code>scipy</code> 包中，可以使用 <code>scipy.misc.imread</code> 来对图片进行读取，
而后来的 <code>scipy</code> 包中可以使用 <code>scipy.imageio.imread</code> 来取代，但是
<code>scipy.imageio.imread</code> 返回的数组类型是 <code>scipy.imageio.core.util.Array</code> ，而不是
常用的 <code>numpy.ndarray</code> 数组，因此可以使用 <code>matplotlib.pyplot.imread</code> 对图片进行读取并返
回 <code>numpy.ndarray</code> 格式。但是最新的 <code>scipy</code> 版本中，也没有了 <code>imageio</code> 模块；而
 <code>matplotlib.pyplot.imread</code> 支持的格式并不是很多，需要可以使用 <code>pillow</code> 。
</p>
</div>
<div id="outline-container-org59898b4" class="outline-3">
<h3 id="org59898b4">图片预览</h3>
<div class="outline-text-3" id="text-org59898b4">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">from</span> PIL <span style="color: #859900;">import</span> Image
<span style="color: #6c71c4;">kieen</span> = Image.<span style="color: #268bd2;">open</span>(<span style="color: #2aa198;">"./images/cifar-on-ann-cat.jpg"</span>)
<span style="color: #405A61;"># </span><span style="color: #405A61;">kieen = plt.imread("./images/cifar-on-ann-cat.jpg")</span>
<span style="color: #6c71c4;">puppy</span> = Image.<span style="color: #268bd2;">open</span>(<span style="color: #2aa198;">"./images/cifar-on-ann-dog.jpg"</span>)
plt.figure(figsize=(10.0, 8.0))
plt.subplot(1, 2, 1)
plt.imshow(kieen)
plt.xticks([])
plt.yticks([])
plt.subplot(1, 2, 2)
plt.imshow(puppy)
plt.xticks([])
plt.yticks([])
plt.show()
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-834866.png" alt="cifar-on-ann-834866.png">

</figure>
</div>
</div>

<div id="outline-container-org71c6cb1" class="outline-3">
<h3 id="org71c6cb1">裁剪</h3>
<div class="outline-text-3" id="text-org71c6cb1">
<p>
由于图片分辨率为1277x1920不是方阵，这里先对图片进行裁剪成方阵。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">kieen_cropped</span> = kieen.crop((0, 0, kieen.size[0], kieen.size[0]))
<span style="color: #6c71c4;">puppy_cropped</span> = puppy.crop((0, 0, puppy.size[0], puppy.size[0]))
plt.subplot(1, 2, 1)
plt.imshow(kieen_cropped)
plt.axis(<span style="color: #2aa198;">'off'</span>)
plt.subplot(1, 2, 2)
plt.imshow(puppy_cropped)
plt.axis(<span style="color: #2aa198;">'off'</span>)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-689444.png" alt="cifar-on-ann-689444.png">

</figure>
</div>
</div>

<div id="outline-container-org8a11818" class="outline-3">
<h3 id="org8a11818">重设大小</h3>
<div class="outline-text-3" id="text-org8a11818">
<p>
选择一个更小的图片进行试验。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">img_size</span> = 200
<span style="color: #6c71c4;">kieen_small</span> = kieen.resize((img_size, img_size))
<span style="color: #6c71c4;">puppy_small</span> = puppy.resize((img_size, img_size))
<span style="color: #6c71c4;">kieen_array</span> = np.array(kieen_small)
<span style="color: #6c71c4;">puppy_array</span> = np.array(puppy_small)

<span style="color: #6c71c4;">x</span> = np.zeros((2, 3, img_size, img_size))
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#23558;RGB&#32500;&#25918;&#22312;&#21069;&#38754;</span>
<span style="color: #6c71c4;">x</span>[0, :, :, :] = kieen_array.transpose((2, 0, 1))
<span style="color: #6c71c4;">x</span>[1, :, :, :] = puppy_array.transpose((2, 0, 1))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgccbfbcc" class="outline-3">
<h3 id="orgccbfbcc">生成权重矩阵（过滤器）</h3>
<div class="outline-text-3" id="text-orgccbfbcc">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #405A61;"># </span><span style="color: #405A61;">&#19968;&#20849;&#20004;&#20010;&#36807;&#28388;&#22120;&#65292;&#27599;&#19968;&#20010;&#20026;3x3x3</span>
<span style="color: #6c71c4;">w</span> = np.zeros((2, 3, 3, 3))

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#31532;&#19968;&#20010;&#65292;&#21033;&#29992;&#30697;&#38453;&#23545;&#22270;&#29255;&#36827;&#34892;&#36716;&#21464;</span>
<span style="color: #6c71c4;">w</span>[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]] <span style="color: #405A61;"># </span><span style="color: #405A61;">red</span>
<span style="color: #6c71c4;">w</span>[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]] <span style="color: #405A61;"># </span><span style="color: #405A61;">green</span>
<span style="color: #6c71c4;">w</span>[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]] <span style="color: #405A61;"># </span><span style="color: #405A61;">blue</span>

<span style="color: #6c71c4;">w</span>[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]] <span style="color: #405A61;"># </span><span style="color: #405A61;">blue</span>
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#20559;&#24046;</span>
<span style="color: #6c71c4;">b</span> = np.array([0, 128])
</pre>
</div>
</div>
</div>
<div id="outline-container-org842bd12" class="outline-3">
<h3 id="org842bd12">卷积操作</h3>
<div class="outline-text-3" id="text-org842bd12">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">out</span>, <span style="color: #6c71c4;">_</span> = forward_pass(x, w, b, 1, 1)

<span style="color: #859900;">def</span> <span style="color: #268bd2;">imshow_helper</span>(img, normalize=<span style="color: #d33682;">True</span>):
    <span style="color: #35a69c;">'''predigest the plot command'''</span>
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#24402;&#19968;&#21270;</span>
    <span style="color: #859900;">if</span> normalize:
        <span style="color: #6c71c4;">img_max</span>, <span style="color: #6c71c4;">img_min</span> = np.<span style="color: #268bd2;">max</span>(img), np.<span style="color: #268bd2;">min</span>(img)
        <span style="color: #6c71c4;">img</span> = 225.0 * (img - img_min) / (img_max - img_min)
    plt.imshow(img.astype(<span style="color: #2aa198;">'uint8'</span>))
    plt.gca().axis(<span style="color: #2aa198;">'off'</span>)

plt.figure(figsize=(15, 10))
<span style="color: #405A61;"># </span><span style="color: #405A61;">kieen</span>
plt.subplot(2, 4, 1)
imshow_helper(kieen_array, normalize=<span style="color: #d33682;">False</span>)
plt.title(<span style="color: #2aa198;">'original'</span>)
plt.subplot(2, 4, 2)
imshow_helper(kieen_array, normalize=<span style="color: #d33682;">True</span>)
plt.title(<span style="color: #2aa198;">'normalize'</span>)
plt.subplot(2, 4, 3)
imshow_helper(out[0, 0])
plt.title(<span style="color: #2aa198;">'grayscale'</span>)
plt.subplot(2, 4, 4)
imshow_helper(out[0, 1])
plt.title(<span style="color: #2aa198;">'edges'</span>)

<span style="color: #405A61;"># </span><span style="color: #405A61;">puppy</span>
plt.subplot(2, 4, 5)
imshow_helper(puppy_array, normalize=<span style="color: #d33682;">False</span>)
plt.subplot(2, 4, 6)
imshow_helper(puppy_array, normalize=<span style="color: #d33682;">True</span>)
plt.subplot(2, 4, 7)
imshow_helper(out[1, 0])
plt.subplot(2, 4, 8)
imshow_helper(out[1, 1])
plt.show()
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-321220.png" alt="cifar-on-ann-321220.png">

</figure>
</div>
</div>
</div>
<div id="outline-container-org6dc826e" class="outline-2">
<h2 id="org6dc826e">两层神经网络</h2>
<div class="outline-text-2" id="text-org6dc826e">
<p>
<code>input -&gt; fully connected layer -&gt; ReLU(no linear) -&gt; fully connected
layer(class scores) -&gt; softmax(loss function).</code>
</p>

<p>
通过线性（全连接层）计算出来的得分，经过处理（分类中，寻找最大的得分的类作为预测
结果）后，将所预测的结果与实际的目标进行比较，如果差异很小，则代表当前的参数很好。
(我们希望预测结果与目标接近，即差异越小效果越优)。而如果差异很大，则我们想将该信
息传递给前面的权重参数（比如，当前差异很大，希望减少权重），这就是反向传播的原理。
而在传播过程前，我们需要将“很大差异”转便成可以接受的数值。这就是损失函数的工作。
</p>

<p>
其应该具有几个要求：
</p>
<ul class="org-ul">
<li>可以通过求导知道权重更新的方向(可导)</li>
<li>如果当前“差异”很小，应当保持前面的权重不变。</li>
</ul>

<p>
在下面的例子中使用的损失函数是 <code>Softmax</code> 函数。\(L_i =
-log(\frac{f_{y_j}}{\sum_j e^{f_j}})\)， 其中\(f_{y_i}\)代表着目前的正确类的
<code>scores</code> 。
</p>
</div>
<div id="outline-container-org734bbba" class="outline-3">
<h3 id="org734bbba">模型</h3>
<div class="outline-text-3" id="text-org734bbba">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">class</span> <span style="color: #b58900;">TwoLayerNN</span>(<span style="color: #268bd2;">object</span>):
    <span style="color: #859900;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900;">self</span>, input_size, hidden_size, output_size, std=1e-4):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - input_size: The input dataset every sample dimension muliplay.</span>
<span style="color: #35a69c;">        - hidden_size: The neurons of H in the hidden layers.</span>
<span style="color: #35a69c;">        - output_size: The numbers of classes.</span>
<span style="color: #35a69c;">        - std: The standard variable.</span>
<span style="color: #35a69c;">        Outputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - self.params: (dict) store the parameters of weights, bias.</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #859900;">self</span>.params = {}
        <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W1'</span>] = std * np.random.randn(input_size, hidden_size)
        <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B1'</span>] = np.zeros(hidden_size)
        <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W2'</span>] = std * np.random.randn(hidden_size, output_size)
        <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B2'</span>] = np.zeros(output_size)

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">loss</span>(<span style="color: #859900;">self</span>, x, y=<span style="color: #d33682;">None</span>, reg=0.0):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        Use softmax compute loss</span>
<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - x: The x is the need computer loss value data set..</span>
<span style="color: #35a69c;">        - y: The y is labels parallelism of x.</span>
<span style="color: #35a69c;">        - reg: The lambda of reguarization.</span>
<span style="color: #35a69c;">        Outpus:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - loss: The loss about softmax data loss and regularization loss.</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #6c71c4;">W1</span>, <span style="color: #6c71c4;">B1</span> = <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W1'</span>], <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B1'</span>]
        <span style="color: #6c71c4;">W2</span>, <span style="color: #6c71c4;">B2</span> = <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W2'</span>], <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B2'</span>]
        <span style="color: #6c71c4;">N</span>, <span style="color: #6c71c4;">D</span> = x.shape

        <span style="color: #405A61;"># </span><span style="color: #405A61;">input -&gt; [fc -&gt; relu] -&gt; fc</span>
        <span style="color: #6c71c4;">H1</span> = np.maximum(0, np.dot(x, W1) + B1)
        <span style="color: #405A61;"># </span><span style="color: #405A61;">input -&gt; fc -&gt; relu -&gt; [fc]</span>
        <span style="color: #6c71c4;">scores</span> = np.dot(H1, W2) + B2
        <span style="color: #859900;">if</span> y <span style="color: #859900;">is</span> <span style="color: #d33682;">None</span>:
            <span style="color: #859900;">return</span> scores

        <span style="color: #6c71c4;">scores</span> -= scores.<span style="color: #268bd2;">max</span>()
        <span style="color: #6c71c4;">exp_scores</span> = np.exp(scores)
        <span style="color: #6c71c4;">sum_exp_scores</span> = np.<span style="color: #268bd2;">sum</span>(exp_scores, axis=1)
        <span style="color: #6c71c4;">corr_exp_scores</span> = exp_scores[<span style="color: #268bd2;">range</span>(N), y]
        <span style="color: #6c71c4;">data_loss</span> = (-1) * np.log(corr_exp_scores / sum_exp_scores)
        <span style="color: #6c71c4;">reg_loss</span> = reg * (np.<span style="color: #268bd2;">sum</span>(W1 * W1) + np.<span style="color: #268bd2;">sum</span>(W2 * W2))
        <span style="color: #6c71c4;">loss</span> = np.<span style="color: #268bd2;">sum</span>(data_loss) / N + reg_loss

        <span style="color: #6c71c4;">grads</span> = {}              <span style="color: #405A61;"># </span><span style="color: #405A61;"># backpass gradient</span>
        <span style="color: #6c71c4;">d_scores</span> = exp_scores / sum_exp_scores.reshape(N, 1)
        <span style="color: #6c71c4;">d_scores</span>[<span style="color: #268bd2;">range</span>(N), y] = - (sum_exp_scores - corr_exp_scores) / sum_exp_scores
        <span style="color: #6c71c4;">d_scores</span> /= N
        <span style="color: #6c71c4;">d_H1</span> = d_scores.dot(W2.T)
        <span style="color: #6c71c4;">d_H1</span>[H1 == 0] = 0
        <span style="color: #6c71c4;">d_W2</span> = H1.T.dot(d_scores)
        <span style="color: #6c71c4;">d_B2</span> = d_scores.<span style="color: #268bd2;">sum</span>(axis=0)
        <span style="color: #6c71c4;">d_W1</span> = x.T.dot(d_H1)
        <span style="color: #6c71c4;">d_B1</span> = d_H1.<span style="color: #268bd2;">sum</span>(axis=0)

        <span style="color: #6c71c4;">grads</span>[<span style="color: #2aa198;">'W1'</span>] = d_W1 + reg * W1 * 2
        <span style="color: #6c71c4;">grads</span>[<span style="color: #2aa198;">'W2'</span>] = d_W2 + reg * W2 * 2
        <span style="color: #6c71c4;">grads</span>[<span style="color: #2aa198;">'B1'</span>] = d_B1
        <span style="color: #6c71c4;">grads</span>[<span style="color: #2aa198;">'B2'</span>] = d_B2
        <span style="color: #859900;">return</span> loss, grads

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">train</span>(<span style="color: #859900;">self</span>, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95,
              reg=5e-6, num_iters=100, batch_size=200, verbose=<span style="color: #d33682;">False</span>):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - X: The train data set. (N, D)</span>
<span style="color: #35a69c;">        - y: The train data labels. (N, )</span>
<span style="color: #35a69c;">        - X_val: The validation data set. (N_val, D).</span>
<span style="color: #35a69c;">        - y_val: The validation data labels. (N_val, )</span>
<span style="color: #35a69c;">        - learning_rate: The weight update learning step.</span>
<span style="color: #35a69c;">        - learning_rate_decay: The parameters used to decay the learning rate after each point.</span>
<span style="color: #35a69c;">        - reg: The lambda of regularizaion.</span>
<span style="color: #35a69c;">        - num_iters: The iter numbers of train.</span>
<span style="color: #35a69c;">        - batch_size: Number of sample used each train.</span>
<span style="color: #35a69c;">        - verbose: boolean, whether print details information.</span>
<span style="color: #35a69c;">        Outputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #6c71c4;">num_train</span> = X.shape[0]
        <span style="color: #6c71c4;">each_iter_epoch</span> = <span style="color: #268bd2;">max</span>(num_train / batch_size, 1)

        <span style="color: #6c71c4;">loss_history</span> = []
        <span style="color: #6c71c4;">train_acc_history</span> = []
        <span style="color: #6c71c4;">val_acc_history</span> = []

        <span style="color: #859900;">for</span> i <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(num_iters):
            <span style="color: #6c71c4;">batch_idx</span> = np.random.choice(num_train, batch_size)
            <span style="color: #6c71c4;">x_batch</span> = X[batch_idx]
            <span style="color: #6c71c4;">y_batch</span> = y[batch_idx]

            <span style="color: #6c71c4;">loss</span>, <span style="color: #6c71c4;">grads</span> = <span style="color: #859900;">self</span>.loss(x_batch, y_batch, reg=reg)
            loss_history.append(loss)

            <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W1'</span>] -= learning_rate * grads[<span style="color: #2aa198;">'W1'</span>]
            <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W2'</span>] -= learning_rate * grads[<span style="color: #2aa198;">'W2'</span>]
            <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B1'</span>] -= learning_rate * grads[<span style="color: #2aa198;">'B1'</span>]
            <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B2'</span>] -= learning_rate * grads[<span style="color: #2aa198;">'B2'</span>]

            <span style="color: #859900;">if</span> verbose <span style="color: #859900;">and</span> i % 100 == 0:
                <span style="color: #859900;">print</span>(<span style="color: #2aa198;">"inter number: (%d / %d) loss: %f"</span> %(i, num_iters, loss))

            <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23547;&#25214;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#12290;</span>
            <span style="color: #859900;">if</span> i % each_iter_epoch == 0:
                <span style="color: #6c71c4;">train_acc</span> = (<span style="color: #859900;">self</span>.predict(x_batch) == y_batch).mean()
                <span style="color: #6c71c4;">val_acc</span> = (<span style="color: #859900;">self</span>.predict(X_val) == y_val).mean()
                train_acc_history.append(train_acc)
                val_acc_history.append(val_acc)

                <span style="color: #6c71c4;">learning_rate</span> *= learning_rate_decay

        <span style="color: #859900;">return</span> {
            <span style="color: #2aa198;">"loss_history"</span>: loss_history,
            <span style="color: #2aa198;">"train_acc_history"</span>: train_acc_history,
            <span style="color: #2aa198;">"val_acc_history"</span>: val_acc_history
        }

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">predict</span>(<span style="color: #859900;">self</span>, x):
        <span style="color: #6c71c4;">y_pred</span> = np.argmax(np.dot(np.maximum(0, np.dot(x, <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W1'</span>]) + <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B1'</span>]),
                                  <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'W2'</span>])
                           + <span style="color: #859900;">self</span>.params[<span style="color: #2aa198;">'B2'</span>], axis=1)
        <span style="color: #859900;">return</span> y_pred
</pre>
</div>
</div>
</div>
<div id="outline-container-org6633f21" class="outline-3">
<h3 id="org6633f21">测试</h3>
<div class="outline-text-3" id="text-org6633f21">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">input_size</span> = 4                  <span style="color: #405A61;"># </span><span style="color: #405A61;">&#21333;&#20010;&#26679;&#26412;&#30340;&#32500;&#24230;</span>
<span style="color: #6c71c4;">output_size</span> = 3                 <span style="color: #405A61;"># </span><span style="color: #405A61;">&#36755;&#20986;&#30340;&#31867;&#20010;&#25968;</span>
<span style="color: #6c71c4;">hidden_size</span> = 10                <span style="color: #405A61;"># </span><span style="color: #405A61;">&#38544;&#34255;&#23618;&#20013;&#33410;&#28857;&#20010;&#25968;</span>
<span style="color: #6c71c4;">num_input</span> = 5                   <span style="color: #405A61;"># </span><span style="color: #405A61;">&#26679;&#26412;&#20010;&#25968;</span>

<span style="color: #859900;">def</span> <span style="color: #268bd2;">init_twolayer</span>():
    np.random.seed(0)
    <span style="color: #859900;">return</span> TwoLayerNN(input_size, hidden_size, output_size, std=1e-1)

<span style="color: #859900;">def</span> <span style="color: #268bd2;">init_data</span>():
    np.random.seed(1)
    <span style="color: #6c71c4;">x</span> = 10 * np.random.randn(num_input, input_size)
    <span style="color: #6c71c4;">y</span> = np.array([0, 1, 2, 2, 1])
    <span style="color: #859900;">return</span> x, y

<span style="color: #6c71c4;">net</span> = init_twolayer()
<span style="color: #6c71c4;">net_x</span>, <span style="color: #6c71c4;">net_y</span> = init_data()
<span style="color: #6c71c4;">net_loss</span>, <span style="color: #6c71c4;">_</span> = net.loss(net_x, net_y, reg=0.05)
<span style="color: #268bd2;">abs</span>(net_loss - <span style="color: #268bd2;">float</span>(1.30378789133))
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2c3f6c0" class="outline-2">
<h2 id="org2c3f6c0">训练模型</h2>
<div class="outline-text-2" id="text-org2c3f6c0">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">cifar_net</span> = TwoLayerNN(X_train2d.shape[1], 50, 10)
<span style="color: #6c71c4;">cifar_stats</span> = cifar_net.train(X_train2d, Y_train, X_vali2d, Y_vali,
                              learning_rate=1e-4, learning_rate_decay=0.95,
                              reg=0.25, num_iters=1000, batch_size=200, verbose=<span style="color: #d33682;">False</span>)
</pre>
</div>
<p>
查看模型的准确率。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">print</span>(<span style="color: #2aa198;">"val accurary: %f"</span> %((cifar_net.predict(X_vali2d) == Y_vali).mean()))
</pre>
</div>

<pre class="example">
val accurary: 0.248000

</pre>
</div>
</div>
<div id="outline-container-orgfa921a9" class="outline-2">
<h2 id="orgfa921a9">调整参数</h2>
<div class="outline-text-2" id="text-orgfa921a9">
<p>
通过前面的训练过程中，我们记录了 <code>loss_history, train_acc_history,
val_acc_history</code> 加上从模型的对象中可以获得模型的权重信息。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(9.0, 6.0))
plt.plot(cifar_stats[<span style="color: #2aa198;">'loss_history'</span>])
plt.xlabel(<span style="color: #2aa198;">'x'</span>)
plt.ylabel(<span style="color: #2aa198;">'loss'</span>)
</pre>
</div>

<p>
<img src="./images/cifar-on-ann-602007.png" alt="cifar-on-ann-602007.png">
从上图中，看出损失函数是来回波动下降的，而导致波动的原因可能是学习速率太快。
</p>

<p>
将前面训练的过程结合起来，根据输出结果进行调整参数，寻找最高的准确率。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">hidden_size</span> = <span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">range</span>(100, 1000, 100))
<span style="color: #6c71c4;">learning_rate</span> = [1e-3, 1e-4, 1e-5, 2e-3, 2e-4, 2e-5, 3e-3, 3e-4, 3e-5]
<span style="color: #6c71c4;">reg</span> = np.arange(0, 1, 0.2)

<span style="color: #6c71c4;">cifar_model</span> = <span style="color: #d33682;">None</span>
<span style="color: #6c71c4;">cifar_model_acc</span> = 0
<span style="color: #859900;">for</span> h <span style="color: #859900;">in</span> hidden_size:
    <span style="color: #859900;">for</span> l <span style="color: #859900;">in</span> learning_rate:
        <span style="color: #859900;">for</span> r <span style="color: #859900;">in</span> reg:
            <span style="color: #6c71c4;">net</span> = TwoLayerNN(X_train2d.shape[1], h, 10)
            <span style="color: #6c71c4;">net_stats</span> = net.train(X_train2d, Y_train, X_vali2d, Y_vali,
                                   learning_rate=l, learning_rate_decay=0.95,
                                   reg=r, num_iters=1000, batch_size=200, verbose=<span style="color: #d33682;">False</span>)
            <span style="color: #6c71c4;">acc</span> = net_stats[<span style="color: #2aa198;">'val_acc_history'</span>][-1] <span style="color: #405A61;"># </span><span style="color: #405A61;">the last validation accurary.</span>
            <span style="color: #859900;">if</span> acc &gt; cifar_model_acc:
                <span style="color: #6c71c4;">cifar_model</span> = net
                <span style="color: #6c71c4;">cifar_model_acc</span> = acc
            <span style="color: #859900;">print</span>(<span style="color: #2aa198;">"h: %d, l: %f, r: %f, acc: %f"</span> %(h, l, r, acc, ))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">print</span>(<span style="color: #2aa198;">"The best accurary: %f"</span> %cifar_model_acc)

<span style="color: #6c71c4;">y_pred</span> = cifar_model.predict(X_test2d)
<span style="color: #859900;">print</span>(<span style="color: #2aa198;">"Test acc: %f"</span> %(np.mean(y_pred == Y_test)))
</pre>
</div>

<pre class="example">
The best accurary: 0.421000
Test acc: 0.389000

</pre>
</div>
</div>

<div id="outline-container-org5c079a3" class="outline-2">
<h2 id="org5c079a3">总结</h2>
<div class="outline-text-2" id="text-org5c079a3">
<ol class="org-ol">
<li>神经网络的建模流程是：通过通过前向传播计算最后的得分(scores)，根据损失函数、
得分、真实值计算损失值；根据损失函数的可导性质，计算处于当前输入值(inputs)的
梯度，在根据梯度的方向，方向更新权重，以至于损失值达到最小。</li>
<li>调整参数的流程：提前设定好需要迭代的参数(隐藏层、学习速率、正则参数)，计算模
型在各个参数下的准确率，寻找最高准确率的模型。</li>
</ol>
</div>
</div>

<div id="outline-container-org76ebe04" class="outline-2">
<h2 id="org76ebe04">参考</h2>
<div class="outline-text-2" id="text-org76ebe04">
<ol class="org-ol">
<li><a href="http://cs231n.github.io/">CS231N</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
