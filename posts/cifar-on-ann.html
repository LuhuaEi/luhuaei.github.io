<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-10-29 Tue 13:50 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>使用神经网络训练CIFAR-10</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">使用神经网络训练CIFAR-10</h1>
</header>
<div id="outline-container-org14e4035" class="outline-2">
<h2 id="org14e4035">激励函数</h2>
<div class="outline-text-2" id="text-org14e4035">
</div>
<div id="outline-container-org9129f88" class="outline-3">
<h3 id="org9129f88">为什么需要激励函数？</h3>
<div class="outline-text-3" id="text-org9129f88">
<p>
因为在现实中，有很多的数据都不是可以使用一个线或者一个平面进行分割的，而根据维基
百科的定义，线性函数是指：线性函数是只拥有一个参数的一阶多项式函数。所以需要引进
非线性函数用来解决线性函数的不足。这种位于层中的非线性函数被称为激励函数。
</p>

<p>
从另一个角度，如果没有激励函数，那该网络仅仅能表达线性处理，这样即使具有更多的隐
藏层，其功能都可以用单层的神经网络进行实现，表明，如果没有激励函数为模型提供非线
性转化，那隐藏层是没有作用的。
</p>

<p>
激励函数与损失函数：激励函数用于提供非线性功能，而损失函数用于计算预测结果与实际
结果之间的差异。
</p>

<p>
激励函数作为一个神经元，用于对输入变量进行处理，事实上任何数学函数都可以作为激励函数。
常见的激活函数有以下几个：
</p>
</div>
</div>
<div id="outline-container-orgdf07cc3" class="outline-3">
<h3 id="orgdf07cc3">Sigmoid</h3>
<div class="outline-text-3" id="text-orgdf07cc3">
<p>
非线性函数，取值范围为[0, 1], \(\sigma(x) = \frac{1}{1 + exp(-x)}\) 主要将一个值
归一化，即压缩到0与1之间，根据公式可以看出，大的负数值将会被求值为0，而大的正数
值将会被求值为1。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_sigmoid</span> = <span style="color: #859900;">lambda</span> x: 1.0 / (1.0 + np.exp(-x))
<span style="color: #859900;">def</span> <span style="color: #268bd2;">f_plot</span>(f, x):
    <span style="color: #6c71c4;">y</span> = <span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">map</span>(f, x))
    plt.style.use(<span style="color: #2aa198;">'ggplot'</span>)
    plt.plot(x, y, <span style="color: #2aa198;">'r-'</span>)
    plt.plot([0.0, 0.0], [<span style="color: #268bd2;">min</span>(y) -1, <span style="color: #268bd2;">max</span>(y)+1], <span style="color: #2aa198;">'b-'</span>)
    plt.plot([-10.0, 10.0], [0.0, 0.0], <span style="color: #2aa198;">'b-'</span>)
    plt.xlabel(<span style="color: #2aa198;">'x'</span>)
    plt.ylabel(<span style="color: #2aa198;">'y'</span>)

<span style="color: #6c71c4;">x</span> = np.arange(-10, 10, 0.001)
f_plot(f_sigmoid, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-393527.png" alt="cifar-on-ann-393527.png">

</figure>
</div>
<div id="outline-container-org0f7d2f2" class="outline-4">
<h4 id="org0f7d2f2">缺点：</h4>
<div class="outline-text-4" id="text-org0f7d2f2">
<p>
在 sigmoid 的两端即位于0或者1上的点的梯度都是0(也称为：饱和)，如果一开始所给予过大
的权重，将会直接被判断为1，将不会被学习。函数输出的结果不是以0为中心化的，而且都
是为大于0。
</p>
</div>
</div>
</div>
<div id="outline-container-orgee228bf" class="outline-3">
<h3 id="orgee228bf">tanh(logistic)</h3>
<div class="outline-text-3" id="text-orgee228bf">
<p>
非线性函数，取值范围为[-1, 1]，Sigmod变体，但是为0中心化的，\(tanh(x) =
2\sigma(2x) - 1\)
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_tanh</span> = <span style="color: #859900;">lambda</span> x: 2.0 * f_sigmoid(2.0 * x) - 1.0
f_plot(f_tanh, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-891349.png" alt="cifar-on-ann-891349.png">

</figure>
</div>
</div>

<div id="outline-container-org088268b" class="outline-3">
<h3 id="org088268b">修正线性单元(ReLU)</h3>
<div class="outline-text-3" id="text-org088268b">
<p>
ReLU(Rectified Linear Unit)函数表示成\(ReLU(x) = max(0, x)\)。从斜率的角度看就是，
当x大于0,斜率为1,否则为0。因为ReLU的缺点，所以在使用时，需要小心模型学习速率，如
果很关心学习的速率，可以尝试Leaky ReLU或者Maxout。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_relu</span> = <span style="color: #859900;">lambda</span> x: x <span style="color: #859900;">if</span> x &gt; 0 <span style="color: #859900;">else</span> 0
f_plot(f_relu, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-723822.png" alt="cifar-on-ann-723822.png">

</figure>
</div>

<div id="outline-container-org3cce919" class="outline-4">
<h4 id="org3cce919">优点：</h4>
<div class="outline-text-4" id="text-org3cce919">
<p>
在随机梯度下降中，比tanh/Sigmod的收敛速度更快，并由于其是线性的，不会出现饱和现
象(指当达到一定的阀值后，值不会发生改变，如Sigmod中的，当达到一定程度后，很大的
数求值为1,很小的数求值为0，位于两点的梯度都为0，造成梯度消失(kill gradient)。与
tanh/Sigmod相比，计算量更小，只需要判断，而tanh/Sigmod需要计算指数。
</p>
</div>
</div>
<div id="outline-container-org4903870" class="outline-4">
<h4 id="org4903870">缺点：</h4>
<div class="outline-text-4" id="text-org4903870">
<p>
ReLU很脆弱，如果当具有一个很大的梯度通过一个ReLU神经元时，将会导致这个神经元“死
掉”。这是因为大的梯度，导致权重更新的步伐过快，将可能导致可以通过调整更低的学习速率进行解决。
</p>
</div>
</div>
</div>
<div id="outline-container-org3cc8119" class="outline-3">
<h3 id="org3cc8119">泄漏ReLU(leaky ReLU)</h3>
<div class="outline-text-3" id="text-org3cc8119">
<p>
为了解决ReLU“死掉”问题而设计，当 x&lt;0 时，将给予一个微小的数，而不是0。就如，当
\(x<0 f(x) = ax\) 当 \(x>=0 f(x)=x\), 其中a代表一个常数，在一些方面可以取得很好
的结果，但一些方面结果并不令人满意。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_leaky_relu</span> = <span style="color: #859900;">lambda</span> x: x <span style="color: #859900;">if</span> x &gt;= 0 <span style="color: #859900;">else</span> 0.25*x
f_plot(f_leaky_relu, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-495479.png" alt="cifar-on-ann-495479.png">

</figure>
</div>
</div>

<div id="outline-container-org4d1f99b" class="outline-3">
<h3 id="org4d1f99b">Maxout</h3>
<div class="outline-text-3" id="text-org4d1f99b">
<p>
将多个激励函数进行合并，表达式为: \(max(F_1, F_2, ...)\) ，当后面的\(F_2, ...,
F_n\)为0时退化为ReLU函数，防止梯度消失(饱和)， 有避免ReLu的缺点(因为梯度过大导致
神经元“死掉”)，但这样，其对于每一个神经元必须需要传递两个参数以上的变量。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">f_maxout</span> = <span style="color: #859900;">lambda</span> x: f_sigmoid(x) <span style="color: #859900;">if</span> f_sigmoid(x) &gt; f_relu(x) <span style="color: #859900;">else</span> f_relu(x)
f_plot(f_maxout, x)
</pre>
</div>


<figure>
<img src="./images/cifar-on-ann-890421.png" alt="cifar-on-ann-890421.png">

</figure>
</div>
</div>
</div>
<div id="outline-container-org6ce96fe" class="outline-2">
<h2 id="org6ce96fe">前向传播</h2>
<div class="outline-text-2" id="text-org6ce96fe">
<p>
前向传播就是平时看到的传播模式，就是一个一个接着往下传播。简单的前向传播实现，主
要用于理解前向传播的思想。
</p>
</div>
<div id="outline-container-orgf156b52" class="outline-3">
<h3 id="orgf156b52">前向传播算法</h3>
<div class="outline-text-3" id="text-orgf156b52">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">forward_pass</span>(input_volumes, input_weights, input_biases, stride=1, zero_padding=0):
    <span style="color: #35a69c;">'''input_array(num, channel, width, height) &#20998;&#21035;&#23545;&#24212;&#22270;&#29255;&#30340;&#25968;&#37327;&#65292;&#22270;&#29255;&#30340;&#23618;&#25968;&#65292;&#23485;&#24230;&#65292;&#39640;&#24230;</span>
<span style="color: #35a69c;">    input_weights(num, channel, width, height) &#20998;&#21035;&#23545;&#24212;&#36807;&#28388;&#22120;&#30340;&#20010;&#25968;&#65292;channel&#65292;&#23485;&#24230;&#65292;&#39640;&#24230;</span>
<span style="color: #35a69c;">    channel &#19982; depth &#24182;&#19981;&#19968;&#26679;&#65292;&#21069;&#32773;&#20195;&#34920;&#21333;&#20010;&#26679;&#26412;&#20013;&#30340;Z&#36724;&#65292;&#32780;depth&#20195;</span>
<span style="color: #35a69c;">    &#34920;&#21367;&#31215;&#23618;&#30340;Z&#36724;&#65292;&#21363;&#37319;&#38598;&#22120;&#30340;&#20010;&#25968;&#20063;&#31216;&#20026;&#20869;&#26680;&#30340;&#20010;&#25968;&#12290;</span>
<span style="color: #35a69c;">    '''</span>
    <span style="color: #6c71c4;">x_num</span>, <span style="color: #6c71c4;">x_channel</span>, <span style="color: #6c71c4;">x_height</span>, <span style="color: #6c71c4;">x_width</span> = input_volumes.shape
    <span style="color: #6c71c4;">f_num</span>, <span style="color: #6c71c4;">_</span>, <span style="color: #6c71c4;">f_height</span>, <span style="color: #6c71c4;">f_width</span> = input_weights.shape
    <span style="color: #6c71c4;">out_height</span> = (x_height - f_height + 2*zero_padding) // stride + 1
    <span style="color: #6c71c4;">out_width</span> = (x_width - f_width + 2*zero_padding) // stride + 1

    <span style="color: #6c71c4;">X</span> = np.pad(input_volumes, ((0, 0), (0, 0), (zero_padding, zero_padding), (zero_padding, zero_padding)),
               <span style="color: #2aa198;">"constant"</span>, constant_values=0)

    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#27599;&#19968;&#20010;&#37319;&#38598;&#22120;&#21482;&#20250;&#20135;&#29983;&#19968;&#20010;&#20108;&#32500;&#30340;&#25968;&#32452;</span>
    <span style="color: #6c71c4;">dout</span> = np.zeros((x_num, f_num, out_height, out_width))
    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23545;&#26679;&#26412;&#36845;&#20195;&#65292;&#21363;&#23545;&#27599;&#19968;&#24352;&#22270;&#29255;&#36827;&#34892;&#36845;&#20195;</span>
    <span style="color: #859900;">for</span> n <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(x_num):
        <span style="color: #405A61;"># </span><span style="color: #405A61;">&#23545;&#22810;&#20010;&#37319;&#38598;&#22120;&#36827;&#34892;&#36845;&#20195;&#37319;&#38598;</span>
        <span style="color: #859900;">for</span> f <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(f_num):
            <span style="color: #859900;">for</span> y <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(0, out_height):
                <span style="color: #859900;">for</span> x <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(0, out_width):
                    <span style="color: #405A61;"># </span><span style="color: #405A61;">&#21033;&#29992;&#36755;&#20986;&#30340;&#32500;&#24230;&#65292;&#21453;&#25512;&#37319;&#38598;&#30340;&#21306;&#22495;</span>
                    <span style="color: #6c71c4;">dout</span>[n, f, y, x] = np.<span style="color: #268bd2;">sum</span>(
                        X[n, :, y*stride : y*stride + f_height, x*stride : x*stride + f_width] * input_weights[f]
                    ) + input_biases[f]
    <span style="color: #6c71c4;">cache</span> = (input_volumes, input_weights, input_biases, stride, zero_padding)
    <span style="color: #859900;">return</span> dout, cache
</pre>
</div>
</div>
</div>
<div id="outline-container-org0c648bf" class="outline-3">
<h3 id="org0c648bf">测试数据</h3>
<div class="outline-text-3" id="text-org0c648bf">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">x_shape</span> = (2, 3, 4, 4)
<span style="color: #6c71c4;">w_shape</span> = (3, 3, 4, 4)
<span style="color: #6c71c4;">x</span> = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)
<span style="color: #6c71c4;">w</span> = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)
<span style="color: #6c71c4;">b</span> = np.linspace(-0.1, 0.2, num=3)

<span style="color: #405A61;"># </span><span style="color: #405A61;">&#27491;&#30830;&#30340;&#31572;&#26696;</span>
<span style="color: #6c71c4;">correct_out</span> = np.array([[[[-0.08759809, -0.10987781],
                          [-0.18387192, -0.2109216 ]],
                         [[ 0.21027089,  0.21661097],
                          [ 0.22847626,  0.23004637]],
                         [[ 0.50813986,  0.54309974],
                          [ 0.64082444,  0.67101435]]],
                        [[[-0.98053589, -1.03143541],
                          [-1.19128892, -1.24695841]],
                         [[ 0.69108355,  0.66880383],
                          [ 0.59480972,  0.56776003]],
                         [[ 2.36270298,  2.36904306],
                          [ 2.38090835,  2.38247847]]]])
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#35745;&#31639;&#30456;&#23545;&#35823;&#24046;</span>
<span style="color: #6c71c4;">dout</span>, <span style="color: #6c71c4;">_</span> = forward_pass(x, w, b, 2, 1)
<span style="color: #859900;">print</span>(<span style="color: #2aa198;">"relative_error: "</span>, relative_error(dout, correct_out))
</pre>
</div>

<pre class="example">
relative_error:  2.2121476417505994e-08

</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
