<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-12-30 Wed 21:31 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>关于数据预处理</title>
<meta name="generator" content="Org mode">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">关于数据预处理</h1>
</header><p>
关于预处理的一个陷阱是：预处理中计算的统计量不是用整个训练集计算得出，而是利用分
割后的训练集计算得出，并应用其他数据集的预处理上。比如：通过计算得出训练集的平均
数为M，那验证集、测试集也是利用M进行中心化。
</p>
<div id="outline-container-orgdf0b977" class="outline-2">
<h2 id="orgdf0b977">中心化</h2>
<div class="outline-text-2" id="text-orgdf0b977">
<p>
普通的中心化是直接减去平均值。
</p>
</div>
</div>
<div id="outline-container-orgd7aa679" class="outline-2">
<h2 id="orgd7aa679">标准化</h2>
<div class="outline-text-2" id="text-orgd7aa679">
<p>
标准化是将均值变成0,方差为1。\(x = \frac{x - EX}{X_{var}}\)
</p>
</div>
</div>
<div id="outline-container-org421c048" class="outline-2">
<h2 id="org421c048">PCA(主成分分析)</h2>
<div class="outline-text-2" id="text-org421c048">
<p>
PCA 的应用前提是变量之间是相关的。通过正交矩阵将对称的协方差阵或者相关阵通过正交
矩阵转换成对角矩阵。
</p>
<ol class="org-ol">
<li>先中心化</li>
<li>计算协方差矩阵，协方差用来衡量两个变量的总体误差，当两个变量相等时，表示为变
量的方差，所以在协方差矩阵中，对角线就是变量的方差、以及协方差矩阵是对称的、
协方差矩阵是半正定矩阵。</li>
<li>对协方差矩阵进行奇异值分解，通过奇异值分解可以得到协方差矩阵的特征值、特征向
量。特征向量就是正交矩阵中的列向量，而特征值大小代表在对应的方向的长度(特征值
就是对应方向上的标准方差的平方，即方差)。利用np.linalg.svd()求值奇异值，返回
(特征向量、特征值、特征向量(横))</li>
<li>在知道特征向量(相当于新的坐标系)的情况下，通过点积将数据映射到新的坐标上。</li>
<li>通过特征向量可以对数据进行降维处理，因为特征向量每一列代表一个坐标轴，所以我
们可以指定选择特向向量的个数以达到降维的目的。</li>
<li>而根据前面的知识，我们知道特征值的大小代表数据在该方向的宽度，而奇异分解中，
返回的结果是经过排序的，所以我们可以根据问题选择前几个特征向量，这就是所谓的
主成分分析。</li>
</ol>
</div>
</div>
<div id="outline-container-org98d3f17" class="outline-2">
<h2 id="org98d3f17">白化变换(whitening transformation、球化)</h2>
<div class="outline-text-2" id="text-org98d3f17">
<p>
球化的步骤差不多与主成分步骤相同，前面通过奇异分解获得特征值、特征向量。然后通过
点积将数据旋转到新的坐标系上，而白化就是将新坐标系上的数据除以特征值的平方跟、
在前面我们说过，特征值代表着数据在对应坐标轴上的长度(宽度)，而现在将每一个数据除
以相对应的特征值平方跟，就是将其标准化(压缩到[-1, 1])，这就导致整个图形看起来很
像一个球体。需要注意一点是，为了防止特征值中出现0值，可以加上一个微小的常数。
</p>
</div>
</div>
<div id="outline-container-org1a84105" class="outline-2">
<h2 id="org1a84105">权重初始化</h2>
<div class="outline-text-2" id="text-org1a84105">
</div>
<div id="outline-container-org226d1f1" class="outline-4">
<h4 id="org226d1f1">全0初始化</h4>
<div class="outline-text-4" id="text-org226d1f1">
<p>
在经过合理的标准化后，我们认为数据一半是小于0,一半是大于0,那这是假设权重为0将是
一个合理的选择，但是如果将所有的权重都初始化为0,而如果网络中每一个神经元都是相同
的输出，那将会导致参数具有同样的更新。
</p>
</div>
</div>
<div id="outline-container-org7947406" class="outline-4">
<h4 id="org7947406">小的随机值</h4>
<div class="outline-text-4" id="text-org7947406">
<p>
所以我们需要寻找一个接近于0,而又不等于0的数作为权重值，并且这些数是不对称的(各自
不相同)。小的随机值是一个很好的选择。但并不是初始权重值越小越好，一个神经网络层
有一个很小的权重将会导致在反向传播中具有很像的梯度。
</p>
</div>
</div>
<div id="outline-container-org9d78202" class="outline-4">
<h4 id="org9d78202">校准方差</h4>
<div class="outline-text-4" id="text-org9d78202">
<p>
采用随机值，将有一个问题就是方差将会随着神经元的输入的增大而增大(每一次生成的值
都是不同的)。通过校准方差可以将方差标准化为1。这将确保所有的神经元都具有相同的输
出分布和提高网络的收敛概率。这是因为如果确保所有的权重值都同分布，就具有同方差的
性质，
</p>
</div>
</div>
<div id="outline-container-orgcd05e07" class="outline-4">
<h4 id="orgcd05e07">初始化偏差</h4>
<div class="outline-text-4" id="text-orgcd05e07">
<p>
一般使用0作为初始化偏差，但也有使用一个很接近0,但非0的数作为初始偏差。
</p>
</div>
</div>
<div id="outline-container-org980f745" class="outline-4">
<h4 id="org980f745">批量数目</h4>
</div>
</div>
<div id="outline-container-orgb155d4b" class="outline-2">
<h2 id="orgb155d4b">正则化(Regularzation)</h2>
<div class="outline-text-2" id="text-orgb155d4b">
<p>
主要用于防止神经网络过拟合(overfiting)，而根据奥卡姆剃刀原理，可以通过降低模型的
复杂度来防止过拟合。也就是说我们不仅仅是要以最小损失为目标，而是要以最小损失和复
杂度为目标。
</p>
</div>
<div id="outline-container-org169067f" class="outline-4">
<h4 id="org169067f">L2正则化</h4>
<div class="outline-text-4" id="text-org169067f">
<p>
\(\frac{1}{\lambda}W^2\)其中\(\lambda\)为正则化率，前面乘以\(\frac{1}{2}\)是用于
消除求梯度带来的2。L2方法对离群值的惩罚很重，而对于接近于0的几乎没有什么惩罚。这
是因为\(x^2\)是曲线上升的。
</p>
</div>
</div>
<div id="outline-container-orge97c35d" class="outline-4">
<h4 id="orge97c35d">L1正则化</h4>
<div class="outline-text-4" id="text-orge97c35d">
<p>
利用L2正则化，并不能令权重为0,而L1表示为\(\lambda|W|\)，其可以导致权重为0,而当权
重为0时，会使相对应的特征从模型中移除，即无论输入什么值，输出都为0。对于一个稀疏
矩阵来说，其几乎不可能对噪点输入产生反应。L1适用于特征变量的选择，而L2适用于正规
化。
</p>
</div>
</div>
<div id="outline-container-org4358535" class="outline-4">
<h4 id="org4358535">最大约束(Max norm constraints)</h4>
<div class="outline-text-4" id="text-org4358535">
<p>
指定一个绝对的上界，使用设计好的梯度下降对其约束。在实践过程中，一般的流程是，通
过正常的方式更新参数，然后判断权重是否处于给定的范围内。对于某些问题可以获得很好
的结果，但是如果一个模型所给予的学习速率很高，将可能会导致总是“超界”导致这个模
型继续向下推进。
</p>
</div>
</div>
<div id="outline-container-org668023d" class="outline-4">
<h4 id="org668023d">Dropout</h4>
<div class="outline-text-4" id="text-org668023d">
<p>
Dropout定义为：一种简单的方式防止过拟合。Dropout中，一个神经元仅仅有p的概率是激
活的，Dropout可以理解成一个简化版的全链接层。
</p>
</div>
</div>
<div id="outline-container-orge77b812" class="outline-4">
<h4 id="orge77b812">前向传播中的噪点</h4>
</div>
<div id="outline-container-org06a5e15" class="outline-4">
<h4 id="org06a5e15">偏差正规化</h4>
</div>
<div id="outline-container-org335c0e1" class="outline-4">
<h4 id="org335c0e1">每层正规化</h4>
</div>
</div>
<div id="outline-container-org9874b3b" class="outline-2">
<h2 id="org9874b3b">损失函数</h2>
<div class="outline-text-2" id="text-org9874b3b">
<p>
训练目标：最小化(数据损失+复杂度)，通过正则化可以降低模型的复杂度，防止过拟合的
情况发生。对于一个监督学习来说，一个样本的损失值就是渴望值(真实值)与预测值之间的
差距。而整个模型的损失就是模型的平均损失\(L = \frac{1}{N}\sum_iL_i\)
</p>

<p>
当分类的类很多时，使用等级Softmax效果可能会更好，
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
