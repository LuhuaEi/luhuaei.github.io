<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-12-30 Wed 21:31 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>线性分类器训练CIFAR(02)</title>
<meta name="generator" content="Org mode">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">线性分类器训练CIFAR(02)</h1>
</header><p>
由于 KNN 每一次分类预测都使用到所有的数据样本，导致计算量很大，不利于模型的快速
部署。在实际应用中，我们希望用户直接输入需要预测的数据，而可以马上输出结果，而不
用重新训练模型。所以，我们需要保留训练过的模型，而可以完全丢弃训练过的数据。
</p>

<p>
一个线性分类器具有两个部分：
</p>
<ul class="org-ul">
<li>scores function(得分函数)，从原始数据计算各个类的得分。</li>
<li>loss function(损失函数)，表示预测值与真实值之间的得分差距。</li>
</ul>

<div id="outline-container-org7d20931" class="outline-2">
<h2 id="org7d20931">scores</h2>
<div class="outline-text-2" id="text-org7d20931">
<p>
权重，在网络中看，就是两两节点之间的连接线，其大小代表着线的粗细，表示着两个点之
间的关系。而一个节点的得分(scores)等于前一层的节点的加权和。
</p>
</div>
<div id="outline-container-org9d21699" class="outline-3">
<h3 id="org9d21699">线性组合得分</h3>
<div class="outline-text-3" id="text-org9d21699">
<div class="org-src-container">
<pre class="src src-jupyter-python">def forward_scores(x, w, b):
    x = x.reshape(x.shape[0], -1)
    scores = x.dot(w) + b
    cache = (x, w, b)
    return scores, cache
</pre>
</div>
</div>
</div>
<div id="outline-container-orga191134" class="outline-3">
<h3 id="orga191134">反向传播</h3>
<div class="outline-text-3" id="text-orga191134">
<div class="org-src-container">
<pre class="src src-jupyter-python">def backward_scores(dscores, cache):
    x, w, b = cache
    dx = dscores.dot(w.T)
    dw = x.T.dot(dscores)
    db = np.sum(dscores, axis=0)
    return dx, dw, db
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org1bb76ee" class="outline-2">
<h2 id="org1bb76ee">loss functions</h2>
<div class="outline-text-2" id="text-org1bb76ee">
<p>
对于一个分类问题，可以计算出正确类与预测类之间的得分(scores)差距。作为一个学习的
过程，我们希望模型前面的权重可以根据得分差距进行调整，经过调整后的权重值，可以使
得模型的损失值更小。
</p>
</div>
<div id="outline-container-orge255616" class="outline-3">
<h3 id="orge255616">多分类支持向量机(MSVM)</h3>
<div class="outline-text-3" id="text-orge255616">
<p>
多分类支持向量机从二分类扩展而来，属于监督学习方法中的一种，经过改进，也可以使用
在无监督学习中(支持向量机聚类)。
</p>
</div>
<div id="outline-container-org1f67a6f" class="outline-4">
<h4 id="org1f67a6f">原理</h4>
<div class="outline-text-4" id="text-org1f67a6f">
<p>
\(L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y-j} + \delta) \) 其中\(i\)代表第\(i\)
个样本，\(j\)表示成当前样本所需要预测的类个数，\(y_j\)代表正确类的得分，根据上面
的公式来看，Supports Vector Machine 的原理在于计算当前样本属于每一个类的得分，然
后通过将其他不正确类的得分减去正确类的得分后加上偏差的总和。
</p>
</div>
</div>
<div id="outline-container-org7b4d133" class="outline-4">
<h4 id="org7b4d133">算法实现</h4>
<div class="outline-text-4" id="text-org7b4d133">
<p>
<code>np.reshape(a, newshape</code>-1)= 其中-1代表根据给出的值，推断出后面的值。下面函数用
来计算数据损失部分。整个 <code>SVM</code> 计算出来的损失函数值，并计算梯度。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">def svm_loss(scores, y, delta=1):
    N = scores.shape[0]
    correct_class_scores = scores[list(range(N)), y].reshape(N, -1)
    margins = scores - correct_class_scores + delta
    margins[list(range(N)), y] = 0
    loss = np.sum(np.maximum(margins, 0)) / N

    dscores = np.zeros(scores.shape)
    # if margins &gt; 0, the \frac{d L_i}{d y_j} = 1
    dscores[margins &gt; 0] = 1
    # if margins &gt; 0, the \frac{d L_i}{d y_i} = \sum -1
    dscores[list(range(N)), y] -= np.sum(margins &gt; 0, axis=1)
    dscores /= N
    return loss, dscores
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgba7608d" class="outline-3">
<h3 id="orgba7608d">Softmax classifier</h3>
<div class="outline-text-3" id="text-orgba7608d">
<p>
对数Softmax 函数为二项分布Logistic回归的推广，使用于多分类，当仅仅有两个类时，退化为
Logistic回归。Logistic输出的结果是两个分类的对数发生比。
</p>

<p>
Softmax classifier 结合交互熵(cross-entropy)和Softmax函数。根据定义softmax的定义
应该为 \(S_i = \frac{ e^{f_{y_i}} }{ \sum_k e^{f_{y_k}}}\), 而在加上交互熵\(C
= - \sum_i y_i log scores_i\)， \(y_i\)表示当前正确的类。\(f_{y_i}\)表示正确类的
得分。当前正确的类表示为1, 否则为0。
</p>

<p>
对Softmax classifier求导的过程中，可以分情况求导\(\frac{d L_i}{d score_j}\)，一
种情况为: \(i = j\)，还有一种情况是：\(i \neq j\)，再进行合并。
</p>

<p>
由于\(e^{f_{y_i}}\)会导致很大的数，从而使得计算机在计算时不稳定，所以我们需要对
其进行正规化，对\(S_i\)方程上下同乘一个常数\(C\)，C为任意参数，普遍的选择\(C =
-max_j f_j\)，使得最大的得分为0。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">def softmax_loss(scores, Y):
    N = scores.shape[0]
    scores -= np.max(scores, axis=1, keepdims=True)
    exp_scores_sum = np.sum(np.exp(scores), axis=1, keepdims=True)

    # np.log(np.exp(scores) / exp_scores_sum)
    log_probs = scores - np.log(exp_scores_sum)
    loss = -np.sum(log_probs[list(range(N)), Y]) / N

    probs = np.exp(log_probs)
    dscores = probs.copy()
    # if i = j, \frac{d L_i}{d e^{scores_i}} = 0
    # because the max value is become 0, then exp(0) = 1
    # if i != j, will equal itself.
    dscores[list(range(N)), Y] -= 1
    dscores /= N

    return loss, dscores
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org2b8bf09" class="outline-2">
<h2 id="org2b8bf09">线性分类器</h2>
<div class="outline-text-2" id="text-org2b8bf09">
<div class="org-src-container">
<pre class="src src-jupyter-python">class LinerClassifier():
    def __init__(self):
        self.W = None
        self.b = None

    def train(self, X, Y, class_num, learning_rate=1e-3, regularization=1e-5, num_iters=100,
              batch_size=128, verbose=False):
        '''
        Inputs:
        ------------------------------------------------------------
        - X: (N, D) train set.
        - Y: (N, 1) label set.
        - learning_rate: (float) SGD learning rate.
        - regularization: (float) regularization lambda.
        - num_iters: (integer) SGD iters num.
        - batch_size: (integer) SGD splits each batch size.
        - verbose: (boolen) whether print details infomations.
        Outputs:
        ------------------------------------------------------------
        - loss_history: (list).
        '''
        x_num, x_dim = X.shape
        if self.W is None:
            self.W = 0.001 * np.random.randn(x_dim, class_num)
        if self.b is None:
            self.b = np.zeros(class_num)

        loss_history = []
        for i in range(num_iters):
            batch_idx = np.random.choice(x_num, batch_size)
            X_batch = X[batch_idx]
            Y_batch = Y[batch_idx]

            scores, cache = forward_scores(X_batch, self.W, self.b)
            data_loss, dscores = self.loss(scores, Y_batch)
            loss = data_loss + 0.5 * regularization * np.sum(self.W ** 2)

            _, dw, db = backward_scores(dscores, cache)
            dw += regularization * self.W

            loss_history.append(loss)

            self.W += (-1) * learning_rate * dw
            self.b += (-1) * learning_rate * db

            if verbose:
                print('Current iters informarion count: %s  loss: %s' %(i, loss))
        return loss_history

    def loss(self, scores, y):
        pass

    def predict(self, X):
        '''
        Inputs:
        ------------------------------------------------------------
        - X: (N, D) is the test set.
        Outputs:
        ------------------------------------------------------------
        - pred: (N, 1) is the predict label.
        '''
        pred = np.zeros(X.shape[0])
        scores = X.dot(self.W)
        pred = np.argmax(scores, axis=1)
        return pred

class LinerSVM(LinerClassifier):
    def loss(self, scores, y):
        return svm_loss(scores, y)

class LinerSoftmax(LinerClassifier):
    def loss(self, scores, y):
        return softmax_loss(scores, y)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb066dd0" class="outline-2">
<h2 id="orgb066dd0">测试数据</h2>
<div class="outline-text-2" id="text-orgb066dd0">
<div class="org-src-container">
<pre class="src src-jupyter-python"># 将数据添加一组偏差
X_train2d_dev = np.hstack([X_train2d, np.ones((X_train2d.shape[0], 1))])
X_test2d_dev = np.hstack([X_test2d, np.ones((X_test2d.shape[0], 1))])

svm = LinerSVM()
# train return loss_history
loss_hist = svm.train(X_train2d_dev, Y_train, class_num=10, learning_rate=1e-7, regularization=2.5e4,
                      num_iters=500, verbose=False)
ypred = svm.predict(X_test2d_dev)
svm_acc = np.mean(ypred == Y_test)

softmax = LinerSoftmax()
softmax_loss_history = softmax.train(X_train2d_dev, Y_train, class_num=10, learning_rate=1e-7, regularization=2.5e4,
                                 num_iters=500, verbose=False)
softmax_ypred = softmax.predict(X_test2d_dev)
softmax_acc = np.mean(softmax_ypred == Y_test)
print("svm accurary: %.2f, softmax accurary: %.2f" %(svm_acc, softmax_acc))
</pre>
</div>

<pre class="example">
svm accurary: 0.33, softmax accurary: 0.26
</pre>
</div>
</div>

<div id="outline-container-org757714d" class="outline-2">
<h2 id="org757714d">损失函数可视化</h2>
<div class="outline-text-2" id="text-org757714d">
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(9.0, 6.0))
plt.plot(loss_hist)
plt.plot(softmax_loss_history)
plt.title('Loss function')
plt.ylabel('Loss value')
plt.xlabel('iters num')
plt.show()
</pre>
</div>


<figure id="orged3d7c3">
<img src="./images/cifar-on-linear-classficier-918772.png" alt="cifar-on-linear-classficier-918772.png">

</figure>
</div>
</div>
<div id="outline-container-org4d09fad" class="outline-2">
<h2 id="org4d09fad">reference</h2>
<div class="outline-text-2" id="text-org4d09fad">
<ol class="org-ol">
<li><a href="https://cs231n.github.io/linear-classify/">CS231N linear classifier</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">wiki supports vectors machine</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
