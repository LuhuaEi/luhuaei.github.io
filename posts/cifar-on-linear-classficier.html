<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-05-17 Sun 14:12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>线性分类器训练CIFAR(02)</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">线性分类器训练CIFAR(02)</h1>
</header><p>
由于 KNN 每一次分类预测都使用到所有的数据样本，导致计算量很大，不利于模型的快速
部署。在实际应用中，我们希望用户直接输入需要预测的数据，而可以马上输出结果，而不
用重新训练模型。所以，我们需要保留训练过的模型，而可以完全丢弃训练过的数据。
</p>

<p>
一个线性分类器具有两个部分：
</p>
<ul class="org-ul">
<li>scores function(得分函数)，从原始数据计算各个类的得分。</li>
<li>loss function(损失函数)，表示预测值与真实值之间的得分差距。</li>
</ul>

<div id="outline-container-org5c68c52" class="outline-2">
<h2 id="org5c68c52">scores</h2>
<div class="outline-text-2" id="text-org5c68c52">
<p>
权重，在网络中看，就是两两节点之间的连接线，其大小代表着线的粗细，表示着两个点之
间的关系。而一个节点的得分(scores)等于前一层的节点的加权和。
</p>
</div>
<div id="outline-container-org8e31257" class="outline-3">
<h3 id="org8e31257">线性组合得分</h3>
<div class="outline-text-3" id="text-org8e31257">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">forward_scores</span>(x, w, b):
    <span style="color: #83a598;">x</span> = x.reshape(x.shape[0], -1)
    <span style="color: #83a598;">scores</span> = x.dot(w) + b
    <span style="color: #83a598;">cache</span> = (x, w, b)
    <span style="color: #fb4934;">return</span> scores, cache
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd33a064" class="outline-3">
<h3 id="orgd33a064">反向传播</h3>
<div class="outline-text-3" id="text-orgd33a064">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">backward_scores</span>(dscores, cache):
    <span style="color: #83a598;">x</span>, <span style="color: #83a598;">w</span>, <span style="color: #83a598;">b</span> = cache
    <span style="color: #83a598;">dx</span> = dscores.dot(w.T)
    <span style="color: #83a598;">dw</span> = x.T.dot(dscores)
    <span style="color: #83a598;">db</span> = np.<span style="color: #fe8019;">sum</span>(dscores, axis=0)
    <span style="color: #fb4934;">return</span> dx, dw, db
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgcebf546" class="outline-2">
<h2 id="orgcebf546">loss functions</h2>
<div class="outline-text-2" id="text-orgcebf546">
<p>
对于一个分类问题，可以计算出正确类与预测类之间的得分(scores)差距。作为一个学习的
过程，我们希望模型前面的权重可以根据得分差距进行调整，经过调整后的权重值，可以使
得模型的损失值更小。
</p>
</div>
<div id="outline-container-orgafe8117" class="outline-3">
<h3 id="orgafe8117">多分类支持向量机(MSVM)</h3>
<div class="outline-text-3" id="text-orgafe8117">
<p>
多分类支持向量机从二分类扩展而来，属于监督学习方法中的一种，经过改进，也可以使用
在无监督学习中(支持向量机聚类)。
</p>
</div>
<div id="outline-container-org7c26145" class="outline-4">
<h4 id="org7c26145">原理</h4>
<div class="outline-text-4" id="text-org7c26145">
<p>
\(L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y-j} + \delta) \) 其中\(i\)代表第\(i\)
个样本，\(j\)表示成当前样本所需要预测的类个数，\(y_j\)代表正确类的得分，根据上面
的公式来看，Supports Vector Machine 的原理在于计算当前样本属于每一个类的得分，然
后通过将其他不正确类的得分减去正确类的得分后加上偏差的总和。
</p>
</div>
</div>
<div id="outline-container-org4ba2bb3" class="outline-4">
<h4 id="org4ba2bb3">算法实现</h4>
<div class="outline-text-4" id="text-org4ba2bb3">
<p>
<code>np.reshape(a, newshape</code>-1)= 其中-1代表根据给出的值，推断出后面的值。下面函数用
来计算数据损失部分。整个 <code>SVM</code> 计算出来的损失函数值，并计算梯度。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">svm_loss</span>(scores, y, delta=1):
    <span style="color: #83a598;">N</span> = scores.shape[0]
    <span style="color: #83a598;">correct_class_scores</span> = scores[<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(N)), y].reshape(N, -1)
    <span style="color: #83a598;">margins</span> = scores - correct_class_scores + delta
    <span style="color: #83a598;">margins</span>[<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(N)), y] = 0
    <span style="color: #83a598;">loss</span> = np.<span style="color: #fe8019;">sum</span>(np.maximum(margins, 0)) / N

    <span style="color: #83a598;">dscores</span> = np.zeros(scores.shape)
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">if margins &gt; 0, the \frac{d L_i}{d y_j} = 1</span>
    <span style="color: #83a598;">dscores</span>[margins &gt; 0] = 1
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">if margins &gt; 0, the \frac{d L_i}{d y_i} = \sum -1</span>
    <span style="color: #83a598;">dscores</span>[<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(N)), y] -= np.<span style="color: #fe8019;">sum</span>(margins &gt; 0, axis=1)
    <span style="color: #83a598;">dscores</span> /= N
    <span style="color: #fb4934;">return</span> loss, dscores
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd1a0477" class="outline-3">
<h3 id="orgd1a0477">Softmax classifier</h3>
<div class="outline-text-3" id="text-orgd1a0477">
<p>
对数Softmax 函数为二项分布Logistic回归的推广，使用于多分类，当仅仅有两个类时，退化为
Logistic回归。Logistic输出的结果是两个分类的对数发生比。
</p>

<p>
Softmax classifier 结合交互熵(cross-entropy)和Softmax函数。根据定义softmax的定义
应该为 \(S_i = \frac{ e^{f_{y_i}} }{ \sum_k e^{f_{y_k}}}\), 而在加上交互熵\(C
= - \sum_i y_i log scores_i\)， \(y_i\)表示当前正确的类。\(f_{y_i}\)表示正确类的
得分。当前正确的类表示为1, 否则为0。
</p>

<p>
对Softmax classifier求导的过程中，可以分情况求导\(\frac{d L_i}{d score_j}\)，一
种情况为: \(i = j\)，还有一种情况是：\(i \neq j\)，再进行合并。
</p>

<p>
由于\(e^{f_{y_i}}\)会导致很大的数，从而使得计算机在计算时不稳定，所以我们需要对
其进行正规化，对\(S_i\)方程上下同乘一个常数\(C\)，C为任意参数，普遍的选择\(C =
-max_j f_j\)，使得最大的得分为0。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">softmax_loss</span>(scores, Y):
    <span style="color: #83a598;">N</span> = scores.shape[0]
    <span style="color: #83a598;">scores</span> -= np.<span style="color: #fe8019;">max</span>(scores, axis=1, keepdims=<span style="color: #fabd2f;">True</span>)
    <span style="color: #83a598;">exp_scores_sum</span> = np.<span style="color: #fe8019;">sum</span>(np.exp(scores), axis=1, keepdims=<span style="color: #fabd2f;">True</span>)

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">np.log(np.exp(scores) / exp_scores_sum)</span>
    <span style="color: #83a598;">log_probs</span> = scores - np.log(exp_scores_sum)
    <span style="color: #83a598;">loss</span> = -np.<span style="color: #fe8019;">sum</span>(log_probs[<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(N)), Y]) / N

    <span style="color: #83a598;">probs</span> = np.exp(log_probs)
    <span style="color: #83a598;">dscores</span> = probs.copy()
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">if i = j, \frac{d L_i}{d e^{scores_i}} = 0</span>
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">because the max value is become 0, then exp(0) = 1</span>
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">if i != j, will equal itself.</span>
    <span style="color: #83a598;">dscores</span>[<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(N)), Y] -= 1
    <span style="color: #83a598;">dscores</span> /= N

    <span style="color: #fb4934;">return</span> loss, dscores
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-orga542d75" class="outline-2">
<h2 id="orga542d75">线性分类器</h2>
<div class="outline-text-2" id="text-orga542d75">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">class</span> <span style="color: #8ec07c;">LinerClassifier</span>():
    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">__init__</span>(<span style="color: #fb4934;">self</span>):
        <span style="color: #fb4934;">self</span>.W = <span style="color: #fabd2f;">None</span>
        <span style="color: #fb4934;">self</span>.b = <span style="color: #fabd2f;">None</span>

    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">train</span>(<span style="color: #fb4934;">self</span>, X, Y, class_num, learning_rate=1e-3, regularization=1e-5, num_iters=100,
              batch_size=128, verbose=<span style="color: #fabd2f;">False</span>):
        <span style="color: #ada296;">'''</span>
<span style="color: #ada296;">        Inputs:</span>
<span style="color: #ada296;">        ------------------------------------------------------------</span>
<span style="color: #ada296;">        - X: (N, D) train set.</span>
<span style="color: #ada296;">        - Y: (N, 1) label set.</span>
<span style="color: #ada296;">        - learning_rate: (float) SGD learning rate.</span>
<span style="color: #ada296;">        - regularization: (float) regularization lambda.</span>
<span style="color: #ada296;">        - num_iters: (integer) SGD iters num.</span>
<span style="color: #ada296;">        - batch_size: (integer) SGD splits each batch size.</span>
<span style="color: #ada296;">        - verbose: (boolen) whether print details infomations.</span>
<span style="color: #ada296;">        Outputs:</span>
<span style="color: #ada296;">        ------------------------------------------------------------</span>
<span style="color: #ada296;">        - loss_history: (list).</span>
<span style="color: #ada296;">        '''</span>
        <span style="color: #83a598;">x_num</span>, <span style="color: #83a598;">x_dim</span> = X.shape
        <span style="color: #fb4934;">if</span> <span style="color: #fb4934;">self</span>.W <span style="color: #fb4934;">is</span> <span style="color: #fabd2f;">None</span>:
            <span style="color: #fb4934;">self</span>.W = 0.001 * np.random.randn(x_dim, class_num)
        <span style="color: #fb4934;">if</span> <span style="color: #fb4934;">self</span>.b <span style="color: #fb4934;">is</span> <span style="color: #fabd2f;">None</span>:
            <span style="color: #fb4934;">self</span>.b = np.zeros(class_num)

        <span style="color: #83a598;">loss_history</span> = []
        <span style="color: #fb4934;">for</span> i <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(num_iters):
            <span style="color: #83a598;">batch_idx</span> = np.random.choice(x_num, batch_size)
            <span style="color: #83a598;">X_batch</span> = X[batch_idx]
            <span style="color: #83a598;">Y_batch</span> = Y[batch_idx]

            <span style="color: #83a598;">scores</span>, <span style="color: #83a598;">cache</span> = forward_scores(X_batch, <span style="color: #fb4934;">self</span>.W, <span style="color: #fb4934;">self</span>.b)
            <span style="color: #83a598;">data_loss</span>, <span style="color: #83a598;">dscores</span> = <span style="color: #fb4934;">self</span>.loss(scores, Y_batch)
            <span style="color: #83a598;">loss</span> = data_loss + 0.5 * regularization * np.<span style="color: #fe8019;">sum</span>(<span style="color: #fb4934;">self</span>.W ** 2)

            <span style="color: #83a598;">_</span>, <span style="color: #83a598;">dw</span>, <span style="color: #83a598;">db</span> = backward_scores(dscores, cache)
            <span style="color: #83a598;">dw</span> += regularization * <span style="color: #fb4934;">self</span>.W

            loss_history.append(loss)

            <span style="color: #fb4934;">self</span>.W += (-1) * learning_rate * dw
            <span style="color: #fb4934;">self</span>.b += (-1) * learning_rate * db

            <span style="color: #fb4934;">if</span> verbose:
                <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'Current iters informarion count: %s  loss: %s'</span> %(i, loss))
        <span style="color: #fb4934;">return</span> loss_history

    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">loss</span>(<span style="color: #fb4934;">self</span>, scores, y):
        <span style="color: #fb4934;">pass</span>

    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">predict</span>(<span style="color: #fb4934;">self</span>, X):
        <span style="color: #ada296;">'''</span>
<span style="color: #ada296;">        Inputs:</span>
<span style="color: #ada296;">        ------------------------------------------------------------</span>
<span style="color: #ada296;">        - X: (N, D) is the test set.</span>
<span style="color: #ada296;">        Outputs:</span>
<span style="color: #ada296;">        ------------------------------------------------------------</span>
<span style="color: #ada296;">        - pred: (N, 1) is the predict label.</span>
<span style="color: #ada296;">        '''</span>
        <span style="color: #83a598;">pred</span> = np.zeros(X.shape[0])
        <span style="color: #83a598;">scores</span> = X.dot(<span style="color: #fb4934;">self</span>.W)
        <span style="color: #83a598;">pred</span> = np.argmax(scores, axis=1)
        <span style="color: #fb4934;">return</span> pred

<span style="color: #fb4934;">class</span> <span style="color: #8ec07c;">LinerSVM</span>(LinerClassifier):
    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">loss</span>(<span style="color: #fb4934;">self</span>, scores, y):
        <span style="color: #fb4934;">return</span> svm_loss(scores, y)

<span style="color: #fb4934;">class</span> <span style="color: #8ec07c;">LinerSoftmax</span>(LinerClassifier):
    <span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">loss</span>(<span style="color: #fb4934;">self</span>, scores, y):
        <span style="color: #fb4934;">return</span> softmax_loss(scores, y)
</pre>
</div>
</div>
</div>
<div id="outline-container-org9caee1a" class="outline-2">
<h2 id="org9caee1a">测试数据</h2>
<div class="outline-text-2" id="text-org9caee1a">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">&#23558;&#25968;&#25454;&#28155;&#21152;&#19968;&#32452;&#20559;&#24046;</span>
<span style="color: #83a598;">X_train2d_dev</span> = np.hstack([X_train2d, np.ones((X_train2d.shape[0], 1))])
<span style="color: #83a598;">X_test2d_dev</span> = np.hstack([X_test2d, np.ones((X_test2d.shape[0], 1))])

<span style="color: #83a598;">svm</span> = LinerSVM()
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">train return loss_history</span>
<span style="color: #83a598;">loss_hist</span> = svm.train(X_train2d_dev, Y_train, class_num=10, learning_rate=1e-7, regularization=2.5e4,
                      num_iters=500, verbose=<span style="color: #fabd2f;">False</span>)
<span style="color: #83a598;">ypred</span> = svm.predict(X_test2d_dev)
<span style="color: #83a598;">svm_acc</span> = np.mean(ypred == Y_test)

<span style="color: #83a598;">softmax</span> = LinerSoftmax()
<span style="color: #83a598;">softmax_loss_history</span> = softmax.train(X_train2d_dev, Y_train, class_num=10, learning_rate=1e-7, regularization=2.5e4,
                                 num_iters=500, verbose=<span style="color: #fabd2f;">False</span>)
<span style="color: #83a598;">softmax_ypred</span> = softmax.predict(X_test2d_dev)
<span style="color: #83a598;">softmax_acc</span> = np.mean(softmax_ypred == Y_test)
<span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">"svm accurary: %.2f, softmax accurary: %.2f"</span> %(svm_acc, softmax_acc))
</pre>
</div>

<pre class="example">
svm accurary: 0.33, softmax accurary: 0.26
</pre>
</div>
</div>

<div id="outline-container-orgcb85f51" class="outline-2">
<h2 id="orgcb85f51">损失函数可视化</h2>
<div class="outline-text-2" id="text-orgcb85f51">
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(9.0, 6.0))
plt.plot(loss_hist)
plt.plot(softmax_loss_history)
plt.title(<span style="color: #b8bb26;">'Loss function'</span>)
plt.ylabel(<span style="color: #b8bb26;">'Loss value'</span>)
plt.xlabel(<span style="color: #b8bb26;">'iters num'</span>)
plt.show()
</pre>
</div>


<figure>
<img src="./images/cifar-on-linear-classficier-918772.png" alt="cifar-on-linear-classficier-918772.png">

</figure>
</div>
</div>
<div id="outline-container-orgaffd6b0" class="outline-2">
<h2 id="orgaffd6b0">reference</h2>
<div class="outline-text-2" id="text-orgaffd6b0">
<ol class="org-ol">
<li><a href="https://cs231n.github.io/linear-classify/">CS231N linear classifier</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA">wiki supports vectors machine</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
