<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-01-05 Sun 22:27 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>从图片中提出特征(CIFAR)(04)</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8"> 
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">从图片中提出特征(CIFAR)(04)</h1>
</header><blockquote>
<p>
Q: How do you eat an elephant?
A: One bite at a time.
</p>
</blockquote>

<p>
先从图片中提取特征，再根据这些特征用线性分类器进行分类。从前面的文章中，我们可以
知道使用 <code>SVM</code> 对图片进行分类，准确率仅仅35%。
</p>

<p>
在深度学习还没有发展起来的时候，人们使用图像识别的主要途径(pipeline):
</p>
<ul class="org-ul">
<li>input images</li>
<li>preprocess. e.g(resize, normalize)</li>
<li>extract features. e.g(HOG(histogram of oriented gradient), HAAR(haar-like
features, 哈尔特征), SIFT(scale
invariant features transforms descriptor，尺度不变特征变换), SURF(Speed Up
Robust Feature、加速稳健特征)</li>
<li>algorithm learning. e.g(Supports Vectors Machine, Random Forests, ANN)</li>
<li>prediction labels.</li>
</ul>
<p>
可见、对于图片识别的传统建模流程与对一些数据进行分析的流程是类似的。
</p>

<div id="outline-container-org772decf" class="outline-2">
<h2 id="org772decf">HOG(histogram of oriented gradient, 方向梯度直方图)</h2>
<div class="outline-text-2" id="text-org772decf">
<blockquote>
<p>
HOG should capture the texture of the images while ignoring color information.
</p>
</blockquote>
<p>
从上面这个通俗的解释上看，HOG是在忽略图片的颜色，而仅仅提取图片的纹理。可以纹理
在图片中就是依靠不同颜色而表现出来的？忽略颜色还怎么提取纹理？一些模型的提取纹理
就是从区域中比较两元素单元的颜色差。
</p>

<p>
因为对于一张灰色度的图片来说，其也是没有颜色的，但我们可以分辨出灰色度的图片，原
因在于，虽然没有颜色，但是每一个元素的亮度不一样。所以如果两个元素之间的亮度差比
较大，将被视为一个纹理。
</p>

<p>
根据<a href="https://zh.wikipedia.org/wiki/%E6%96%B9%E5%90%91%E6%A2%AF%E5%BA%A6%E7%9B%B4%E6%96%B9%E5%9B%BE">维基百科</a>上，描述HOG的具体实现方法是：将图像分割成小的连通区域(细胞单元)、采
集细胞单元中各个像素点的梯度或者边缘的方向直方图。最后将这些直方图组合起来就可以
构成特征采集器(descriptor)。
</p>

<p>
一个采集器，就是从一个 <code>width x height x channel</code> 的图片中，输出一个长度为 <code>n</code>
的向量或者数据， <code>HOG</code> 采集的结果维度并不是固定不变的。
</p>

<p>
至于那一种特征(feature)的“有用的”，这个根据不同的目的具有不同的选择。
</p>
<blockquote>
<p>
如在辨别一个圆形按钮和方形按钮时，边缘(edges)特征将是有用的，而颜色是没有用的。
</p>
</blockquote>

<p>
而在 <code>H(histogram) O(oriented) G(gradient)</code> 采集器中，选梯度(gradients)的方向
(oriented)的分布(distribution, histogram)作为特征。选择梯度的原因在于，位于边缘
或者角落(密度陡增或者陡降的区域)。而且边缘和角落比起其他区域包含更多关于对象形状
的信息。
</p>
</div>

<div id="outline-container-org269397d" class="outline-3">
<h3 id="org269397d">How to calculate HOG?</h3>
<div class="outline-text-3" id="text-org269397d">
<p>
HOG原始被使用于识别行人(pedestrian detection)。至于关于预处理，由于预处理的对后
面的结果作用不大，可以跳过。为了方便后面的计算，将图片的高度设为400,(至于图片大
小，并没有限制，只要求高度与宽度成比例1:2)
</p>

<p>
<a href="https://pixabay.com/photos/person-human-female-girl-face-864804/">图片来源</a>
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #83a598;">female</span> = plt.imread(<span style="color: #b8bb26;">"/home/luhuaei/blog/src/posts/images/images-extract-features-female.jpg"</span>)
<span style="color: #83a598;">female</span> = female[:400, :, :]
plt.figure(figsize=(9.0, 6.0))
plt.imshow(female)
plt.axis(<span style="color: #b8bb26;">'off'</span>)
</pre>
</div>


<figure>
<img src="./images/image-extract-features-305110.png" alt="image-extract-features-305110.png">

</figure>

<p>
下面对上面的图片计算梯度。使用 <code>np.pad()</code> 函数对矩阵进行扩充。其中 <code>((2, 2))</code> 参
数表示左上角扩充2行、右下角扩充2行。 <code>np.pad(np.zeros((2, 2)), ((2, 2)),
mode = 'constant', constant_values = 1)</code> 。使用两个卷积核计算对应x和y的梯度。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #83a598;">kernel_x</span> = np.array([  [+1, 0, -1],
                       [+1, 0, -1],
                       [+1, 0, -1]])
<span style="color: #83a598;">kernel_y</span> = np.array([  [+1, +1, +1],
                       [0, 0, 0],
                       [-1, -1, -1]])
<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">rgb2gray</span>(image):
      <span style="color: #fb4934;">return</span> np.dot(image, [0.299, 0.587, 0.144])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">convolution2d</span>(image, kernel, zero_padding=0, stride=1, grayscale=<span style="color: #fabd2f;">True</span>, keep_channel=<span style="color: #fabd2f;">False</span>):
    <span style="color: #ada296;">'''</span>
<span style="color: #ada296;">    image: The array is 3 dimensional.(channel, height, width)</span>
<span style="color: #ada296;">    kernel: The kenels.</span>
<span style="color: #ada296;">    grayscale: bool, represent whether convert RGB to grayscale.</span>
<span style="color: #ada296;">    keep_channel: bool, represent whether keep channel output.(only grayscale is False)</span>
<span style="color: #ada296;">    '''</span>
    <span style="color: #fb4934;">if</span> grayscale:
        <span style="color: #83a598;">image</span> = rgb2gray(image.transpose(1, 2, 0))
        <span style="color: #83a598;">image</span> = np.atleast_3d(image).transpose(2, 0, 1)
    <span style="color: #fb4934;">elif</span> image.ndim == 2:
        <span style="color: #83a598;">image</span> = np.atleast_3d(image).transpose(2, 0, 1)

    <span style="color: #83a598;">i_channel</span>, <span style="color: #83a598;">i_height</span>, <span style="color: #83a598;">i_width</span> = image.shape
    <span style="color: #fb4934;">if</span> <span style="color: #fb4934;">not</span> keep_channel:
        <span style="color: #83a598;">i_channel</span> = 1

    <span style="color: #83a598;">k_height</span>, <span style="color: #83a598;">k_width</span> = kernel.shape

    <span style="color: #83a598;">o_width</span> = (i_width - k_width + 2 * zero_padding ) // stride + 1
    <span style="color: #83a598;">o_height</span> = (i_height - k_height + 2 * zero_padding) // stride + 1

    <span style="color: #fb4934;">if</span> zero_padding:
        <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">pad_width: if only one tuple, will execute all axis, if provide three, will control each axis.</span>
        <span style="color: #83a598;">image</span> = np.pad(image, ((0, 0), (zero_padding, zero_padding), (zero_padding, zero_padding)),
                       mode=<span style="color: #b8bb26;">'constant'</span>, constant_values=0)
    <span style="color: #83a598;">rest</span> = np.zeros((i_channel, o_height, o_width))

    <span style="color: #fb4934;">for</span> o_h <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_height):
        <span style="color: #fb4934;">for</span> o_w <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_width):
            <span style="color: #83a598;">rect</span> = image[:, o_h:(o_h * stride) + k_height, o_w:(o_w * stride) + k_width] * kernel
            <span style="color: #fb4934;">if</span> <span style="color: #fb4934;">not</span> keep_channel <span style="color: #fb4934;">and</span> <span style="color: #fb4934;">not</span> grayscale:
                <span style="color: #83a598;">rest</span>[:, o_h, o_w] = np.<span style="color: #fe8019;">sum</span>(np.<span style="color: #fe8019;">max</span>(rect, axis=0))
            <span style="color: #fb4934;">else</span>:
                <span style="color: #83a598;">test</span> = np.<span style="color: #fe8019;">sum</span>(rect, axis=(1, 2))
                <span style="color: #83a598;">rest</span>[:, o_h, o_w] = test
    <span style="color: #fb4934;">return</span> rest

<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">computer_oriented</span>(gx, gy):
    <span style="color: #fb4934;">return</span> np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90
</pre>
</div>


<p>
下面，我们对图片计算卷积后的梯度，灰色度、颜色(但只取最大的channel)，保留三层
channel的。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #83a598;">female_gx</span> = convolution2d(female.transpose(2, 0, 1), kernel_x)
<span style="color: #83a598;">female_gy</span> = convolution2d(female.transpose(2, 0, 1), kernel_y)
<span style="color: #83a598;">female_gradient</span> = np.sqrt(np.square(female_gx) + np.square(female_gy))
<span style="color: #83a598;">female_oriented</span> = computer_oriented(female_gy, female_gx)

<span style="color: #83a598;">female_gxc</span> = convolution2d(female.transpose(2, 0, 1), kernel_x, grayscale=<span style="color: #fabd2f;">False</span>)
<span style="color: #83a598;">female_gyc</span> = convolution2d(female.transpose(2, 0, 1), kernel_y, grayscale=<span style="color: #fabd2f;">False</span>)
<span style="color: #83a598;">female_gradientc</span> = np.sqrt(np.square(female_gxc) + np.square(female_gyc))
<span style="color: #83a598;">female_orientedc</span> = computer_oriented(female_gyc, female_gxc)

<span style="color: #83a598;">female_gxcc</span> = convolution2d(female.transpose(2, 0, 1), kernel_x, grayscale=<span style="color: #fabd2f;">False</span>, keep_channel=<span style="color: #fabd2f;">True</span>)
<span style="color: #83a598;">female_gycc</span> = convolution2d(female.transpose(2, 0, 1), kernel_y, grayscale=<span style="color: #fabd2f;">False</span>, keep_channel=<span style="color: #fabd2f;">True</span>)
<span style="color: #83a598;">female_gradientcc</span> = np.sqrt(np.square(female_gxcc) + np.square(female_gycc))
<span style="color: #83a598;">female_orientedcc</span> = computer_oriented(female_gycc, female_gxcc)
</pre>
</div>

<p>
对梯度进行可视化，梯度大的地方代表着亮度越大。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(15.0, 10.0))
plt.subplot(1, 3, 1)
plt.imshow(female_gx[0].astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'gx'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 2)
plt.imshow(female_gy[0].astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'gy'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 3)
plt.imshow(female_gradient[0].astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'combine x and y'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)
</pre>
</div>


<figure>
<img src="./images/image-extract-features-72804.png" alt="image-extract-features-72804.png">

</figure>

<p>
第一张为 <code>gx</code> 为x方向的梯度，第二张为 <code>gy</code> 为y轴方向的梯度。从图片中可以看出 <code>gx</code>
对于垂直的线比较明显，而 <code>gy</code> 对于水平的线比较明显。而第三张可以看出很好的描绘出
整个人的轮廓。
</p>

<p>
对于一个具有颜色的图片来说，将具有三个channel的梯度，选择最大的即代表最大梯度。
梯度中的值为[-255, 255]之间，需要转变成[0, 255]之间，而使用 <code>uint8</code> 可以将负的转
变成无符号的 <code>int8</code> 类型确保为正数。而将其加上255,再除以2,可以将负的元素都变成
[0, 125]之间，而0-125就是表示成灰色度。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(15.0, 15.0))
plt.subplot(1, 3, 1)
plt.imshow((female_gxc[0] + 255)/2)
plt.title(<span style="color: #b8bb26;">'gx'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 2)
plt.imshow((female_gyc[0] + 255)/2)
plt.title(<span style="color: #b8bb26;">'gy'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 3)
plt.imshow((female_gradientc[0] + 255)/2)
plt.title(<span style="color: #b8bb26;">'combine x and y'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)
</pre>
</div>


<figure>
<img src="./images/image-extract-features-705320.png" alt="image-extract-features-705320.png">

</figure>

<p>
具有颜色的梯度可视化。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">plt.figure(figsize=(15.0, 15.0))
plt.subplot(1, 3, 1)
plt.imshow(female_gxcc.transpose(1, 2, 0).astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'gx'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 2)
plt.imshow(female_gycc.transpose(1, 2, 0).astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'gy'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)

plt.subplot(1, 3, 3)
plt.imshow(female_gradientcc.transpose(1, 2, 0).astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.title(<span style="color: #b8bb26;">'magnitude'</span>)
plt.axis(<span style="color: #b8bb26;">'off'</span>)
</pre>
</div>


<figure>
<img src="./images/image-extract-features-682212.png" alt="image-extract-features-682212.png">

</figure>
</div>
</div>

<div id="outline-container-org8c83142" class="outline-3">
<h3 id="org8c83142">Calculate gradient of histogram in 8 x 8 cells</h3>
<div class="outline-text-3" id="text-org8c83142">
<p>
将图片分成8x8的单元块(block)，计算从每一个单元中计算出一个直方值(histogram)。为什么需要就
图片分成8x8的单元块(block)呢？根据维基上的说明，对于人的检测中， 将方向分为9个(bins)通道的效果
最好。如果梯度具有负的，选择0-360度方向，如果梯度为正的，可以选择0-180度。
</p>

<p>
将图片分割成单元块的原因在于：
</p>
<ul class="org-ul">
<li>更加简洁、方便。</li>
<li>如在分为8x8x3=192，一个区块中具有192个元素单元，每一个元素单元具有两个值：
magnitude(female<sub>gradient</sub>) 和 oriented(direction)。所以一共具有8x8x2=128个值。</li>
<li>单元梯度可以具有噪点，而结合一个区块将减少对噪点的敏感度。令模型更健壮
(robust).</li>
</ul>

<p>
至于将图片切割成8x8单元块的原因是，这个根据不同的情形具有不同的选择，如对于一个
仅仅为64x128的图片来说，在行人检测实验中，用来提取特征是足够的。
</p>

<p>
使用0-180度还是使用0-360度？0-180度的被称为无符号梯度(unsigned gradient)，因为带
不带符号表示的梯度大小都是一样的。也就是说，0度与180度被视为是相同的。根据经验，
在行人预测实验中，无符号的比带符号的效果更好。
</p>

<p>
在使用0-180度中，直方图(histogram)被分为9个通道(bins)，分别对应0, 20, 40, &#x2026;,
160。一个区块中的元素单元(这里是8x8的单元块)落(select)在那个分箱取决于区块的方向
(oriented)，而对落(select)在的分箱投票(vote)的值取决于区块的 magnitude 。
</p>

<p>
有趣的是，在投票的过程中(vote)如果一个角度为10度(degree)，其将会处于0-20之间，因此按照10
到两边的距离比例，对其进行切割(split)，分别对两个分箱(bins)进行投票(vote)。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">split_block</span>(arr, cell_num):
    <span style="color: #83a598;">a_channel</span>, <span style="color: #83a598;">a_height</span>, <span style="color: #83a598;">a_width</span> = arr.shape
    <span style="color: #83a598;">o_height</span> = <span style="color: #fe8019;">int</span>(a_height / cell_num)
    <span style="color: #83a598;">o_width</span> = <span style="color: #fe8019;">int</span>(a_width / cell_num)

    <span style="color: #83a598;">rest</span> = np.zeros((a_channel, o_height, o_width, cell_num, cell_num))
    <span style="color: #fb4934;">for</span> o_h <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_height):
        <span style="color: #fb4934;">for</span> o_w <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_width):
            <span style="color: #83a598;">rect</span> = arr[:, o_h * cell_num:(o_h + 1) * cell_num, o_w * cell_num:(o_w + 1) * cell_num]
            <span style="color: #83a598;">rest</span>[:, o_h, o_w, :, :] = rect
    <span style="color: #fb4934;">return</span> rest

<span style="color: #83a598;">female_gradient_block</span> = split_block(female_gradient, 8)
<span style="color: #83a598;">female_oriented_block</span> = split_block(female_oriented, 8)
</pre>
</div>

<p>
描绘第(20, 50)个区块的梯度图，在图中可以看出，在该区块中，梯度大的点分布不均匀，其中
心区域的梯度(颜色衰减方向)的方向大部分为[90-180]之间。而在第(20, 70)图片中看出，大
部分的梯度突变方向为水平[0, 180]方向。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python">
plt.figure(figsize=(9.0, 6.0))
plt.subplot(1, 2, 1)
plt.imshow(female_gradient_block[0, 20, 50, :, :].astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.axis(<span style="color: #b8bb26;">'off'</span>)
plt.title(<span style="color: #b8bb26;">'(20, 50)'</span>)

plt.subplot(1, 2, 2)
plt.imshow(female_gradient_block[0, 20, 70, :, :].astype(<span style="color: #b8bb26;">'uint8'</span>))
plt.axis(<span style="color: #b8bb26;">'off'</span>)
plt.title(<span style="color: #b8bb26;">'(20, 70)'</span>)
</pre>
</div>


<figure>
<img src="./images/image-extract-features-272857.png" alt="image-extract-features-272857.png">

</figure>

<p>
计算各个方块对应的直方图分布。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">magnitude_vote</span>(magnitude_block, oriented_block, method=<span style="color: #b8bb26;">'count'</span>):
    <span style="color: #83a598;">oriented_block</span> = np.where(np.isnan(oriented_block), 0, oriented_block)
    <span style="color: #83a598;">oriented_block</span> = np.where(oriented_block &gt;= 180, oriented_block - 180, oriented_block)
    <span style="color: #83a598;">oriented_block</span> = np.where(oriented_block &lt; -180, oriented_block + 360, oriented_block)
    <span style="color: #83a598;">oriented_block</span> = np.where(oriented_block &lt; 0, oriented_block + 180, oriented_block)
    <span style="color: #83a598;">channel</span>, <span style="color: #83a598;">height</span>, <span style="color: #83a598;">width</span> = magnitude_block.shape

    <span style="color: #83a598;">rest</span> = np.zeros((channel, 9))
    <span style="color: #fb4934;">for</span> c <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(channel):
        <span style="color: #fb4934;">for</span> m, o <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span>(magnitude_block[c, :, :].flatten(), oriented_block[c, :, :].flatten()):
            <span style="color: #83a598;">select_bin</span> = <span style="color: #fe8019;">int</span>(<span style="color: #fe8019;">abs</span>((o // 20) % 9))
            <span style="color: #fb4934;">if</span> method == <span style="color: #b8bb26;">'count'</span>:
                <span style="color: #83a598;">rest</span>[c, select_bin] += 1
                <span style="color: #83a598;">rest</span>[c, <span style="color: #fe8019;">int</span>((select_bin + 1) % 9)] += 1
            <span style="color: #fb4934;">elif</span> method == <span style="color: #b8bb26;">'sum'</span>:
                <span style="color: #83a598;">up_percent</span> = <span style="color: #fe8019;">abs</span>(o - (select_bin + 1) * 20) / 20.0
                <span style="color: #83a598;">rest</span>[c, select_bin] += (1 - up_percent) * (np.<span style="color: #fe8019;">abs</span>(m))
                <span style="color: #83a598;">rest</span>[c, <span style="color: #fe8019;">int</span>((select_bin + 1) % 9)] += up_percent * np.<span style="color: #fe8019;">abs</span>(m)
    <span style="color: #fb4934;">return</span> rest

<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">computer_histogram</span>(magn, orie):
    <span style="color: #83a598;">channel</span>, <span style="color: #83a598;">height</span>, <span style="color: #83a598;">width</span> = magn.shape[:3]
    <span style="color: #83a598;">rest</span> = np.zeros((channel, height, width, 9))

    <span style="color: #fb4934;">for</span> h <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(height):
        <span style="color: #fb4934;">for</span> w <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(width):
            <span style="color: #83a598;">rest</span>[:, h, w, :] = magnitude_vote(magn[:, h, w, :, :], orie[:, h, w, :, :], method=<span style="color: #b8bb26;">'sum'</span>)
    <span style="color: #fb4934;">return</span> rest

<span style="color: #83a598;">female_histogram</span> = computer_histogram(female_gradient_block, female_oriented_block)
</pre>
</div>

<p>
第(20, 50)个方块和(20, 70)方块对应的直方图形。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">plot_female_hist</span>(arr, ax_index=1, method=<span style="color: #b8bb26;">'max'</span>):
    <span style="color: #fb4934;">if</span> method == <span style="color: #b8bb26;">'max'</span>:
        <span style="color: #83a598;">x</span> = np.<span style="color: #fe8019;">max</span>(arr, axis=0)
    <span style="color: #fb4934;">else</span>:
        <span style="color: #83a598;">x</span> = np.<span style="color: #fe8019;">sum</span>(arr, axis=0)
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">ax[ax_index].hist(x, list(range(9)))</span>
    ax[ax_index].bar(<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(9)), x)
    ax[ax_index].set_xticks(<span style="color: #fe8019;">list</span>(<span style="color: #fe8019;">range</span>(9)))
    ax[ax_index].grid(<span style="color: #fabd2f;">True</span>)

<span style="color: #83a598;">fig</span>, <span style="color: #83a598;">ax</span> = plt.subplots(1, 2, figsize=(9.0, 6.0))
plot_female_hist(female_histogram[:, 20, 50, :], 0, <span style="color: #b8bb26;">'max'</span>)
plot_female_hist(female_histogram[:, 20, 70, :], 1, <span style="color: #b8bb26;">'max'</span>)

female_oriented.<span style="color: #fe8019;">max</span>()
</pre>
</div>


<figure>
<img src="./images/image-extract-features-296443.png" alt="image-extract-features-296443.png">

</figure>

<p>
图片的梯度对光线是敏感的。在一个图片中， <code>RGB</code> 处于[0, 255]之间，数值越大，代表
的亮度越大，如果想将图片的亮度调暗一倍，可以将数值除以2。然后对其卷积求值后的梯
度也会减少一半。因此导致直方图中的(magnitude values)也会减少一半。但是，我们渴望
得到的是，无论光线如何，最后得到的结果都是一样的。所以我们需要最其进行归一化
(normalize)，消除不同亮度带来的影响。
</p>

<p>
进行区间归一化的方法有多种，比如 L2、L1 范式等。根据 \(L2 = \sqrt{gx^2 + gy^2}\)
可以直接将 <code>8x8</code> 的方格进行归一化，但更好的方式将一个区域的4个区块连结起来一起归
一化。将结合成4个方块后，将具有 <code>4x9=36</code> 个直方图值。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">hog_normalize</span>(hist):
    <span style="color: #83a598;">channel</span>, <span style="color: #83a598;">height</span>, <span style="color: #83a598;">width</span> = hist.shape[:3]
    <span style="color: #83a598;">o_height</span>, <span style="color: #83a598;">o_width</span> = (height - 2) + 1, (width - 2) + 1

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">the 36 is 4 x 9, because each block have 9 histogram, and we split block 2x2</span>
    <span style="color: #83a598;">rest</span> = np.zeros((channel, o_height, o_width, 36))
    <span style="color: #fb4934;">for</span> o_h <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_height):
        <span style="color: #fb4934;">for</span> o_w <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(o_width):
            <span style="color: #83a598;">rect</span> = hist[:, o_h:o_h + 2, o_w:o_w + 2, :]
            <span style="color: #83a598;">rest</span>[:, o_h, o_w, :] = (rect.flatten() / np.sqrt(np.<span style="color: #fe8019;">sum</span>(np.square(rect)))).reshape(channel, -1)
    <span style="color: #fb4934;">return</span> rest

<span style="color: #83a598;">female_normalize_histogram</span> = hog_normalize(female_histogram)
</pre>
</div>
</div>
</div>

<div id="outline-container-org0a46d4c" class="outline-3">
<h3 id="org0a46d4c">对提取特征用 Supports Vectors Machine 分类</h3>
<div class="outline-text-3" id="text-org0a46d4c">
<p>
这里使用前面定义的线性模型分类中的类函数。也可以使用ANN模型提取的特征建立模型。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">img_hog_feature</span>(img):
    <span style="color: #83a598;">img_conv_gx</span> = convolution2d(img, kernel_x, zero_padding=1)
    <span style="color: #83a598;">img_conv_gy</span> = convolution2d(img, kernel_y, zero_padding=1)

    <span style="color: #83a598;">img_conv_magnitude</span> = np.sqrt(np.square(img_conv_gx) + np.square(img_conv_gy))
    <span style="color: #83a598;">img_conv_oriented</span> = np.arctan(np.divide(img_conv_gy, img_conv_gx))

    <span style="color: #83a598;">img_conv_mblock</span> = split_block(img_conv_magnitude, 8)
    <span style="color: #83a598;">img_conv_oblock</span> = split_block(img_conv_oriented, 8)

    <span style="color: #83a598;">img_conv_histogram</span> = computer_histogram(img_conv_mblock, img_conv_oblock)
    <span style="color: #83a598;">img_hog_histogram</span> = hog_normalize(img_conv_histogram)

    <span style="color: #fb4934;">return</span> img_hog_histogram

<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">cifar_extract_hog</span>(arr):
    <span style="color: #83a598;">N</span>, <span style="color: #83a598;">_</span>, <span style="color: #83a598;">_</span>, <span style="color: #83a598;">_</span> = arr.shape
    <span style="color: #83a598;">rest</span> = []
    <span style="color: #fb4934;">for</span> n <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(N):
        rest.append(img_hog_feature(arr[n, :, :, :]))
        <span style="color: #fb4934;">if</span> n % 100 == 0:
            <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">"execute (%d / %d) %f"</span> %(n, N, n / N))
    <span style="color: #fb4934;">return</span> rest
</pre>
</div>

<p>
对 <code>CIFAR</code> 中提取49000张图片作为训练集，10000张为测试集和1000张图片为验证集。并
对图片进行中心化和标准化， 并为其添加偏差列(最后一列)。 <code>np.hstack(),
np.vstack(), np.dstack()</code> 对列表进行合并，分别为(horizontal, vertical, depth)。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #83a598;">X_train_hog</span> = cifar_extract_hog(X_train)
<span style="color: #83a598;">X_test_hog</span> = cifar_extract_hog(X_test)
<span style="color: #83a598;">X_vali_hog</span> = cifar_extract_hog(X_vali)

<span style="color: #83a598;">X_train_rhog</span> = np.array(X_train_hog).reshape(49000, -1)
<span style="color: #83a598;">X_test_rhog</span> = np.array(X_test_hog).reshape(10000, -1)
<span style="color: #83a598;">X_vali_rhog</span> = np.array(X_vali_hog).reshape(1000, -1)
<span style="color: #83a598;">X_mean_rhog</span> = np.mean(np.array(X_train_rhog), axis=0, keepdims=<span style="color: #fabd2f;">True</span>)

<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_train_rhog -= X_mean_rhog</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_test_rhog -= X_mean_rhog</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_vali_rhog -= X_mean_rhog</span>

<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_std_rhog = np.std(X_train_rhog, axis=0, keepdims=True)</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_train_rhog /= X_std_rhog</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_test_rhog /= X_std_rhog</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_vali_rhog /= X_std_rhog</span>

<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">add bias columns.</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_train_rhog = np.hstack([X_train_rhog, np.ones((X_train_rhog.shape[0], 1))])</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_test_rhog = np.hstack([X_test_rhog, np.ones((X_test_rhog.shape[0], 1))])</span>
<span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">X_vali_rhog = np.hstack([X_vali_rhog, np.ones((X_vali_rhog.shape[0], 1))])</span>
</pre>
</div>

<p>
使用支持向量机对提取的特征进行建模。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #83a598;">learning_rates</span> = [1e-9, 1e-8, 1e-7]
<span style="color: #83a598;">regularization_lambdas</span> = [5e4, 5e5, 5e6]

<span style="color: #83a598;">svm_mode_acc</span> = -1
<span style="color: #83a598;">svm_mode</span> = <span style="color: #fabd2f;">None</span>

<span style="color: #83a598;">svm</span> = LinerSVM()
<span style="color: #83a598;">test_acc</span> = -1
<span style="color: #fb4934;">for</span> lr <span style="color: #fb4934;">in</span> learning_rates:
    <span style="color: #fb4934;">for</span> reg <span style="color: #fb4934;">in</span> regularization_lambdas:
        svm.train(X_train_rhog, Y_train, class_num=10, learning_rate=lr,
                  regularization=reg, num_iters=1500, verbose=<span style="color: #fabd2f;">False</span>)
        <span style="color: #83a598;">val_acc</span> = np.mean(svm.predict(X_vali_rhog) == Y_vali)
        <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">"current validation accurary: %s"</span> %(val_acc, ))
        <span style="color: #fb4934;">if</span> val_acc &gt; svm_mode_acc:
            <span style="color: #83a598;">svm_mode_acc</span> = val_acc
            <span style="color: #83a598;">test_acc</span> = np.mean(svm.predict(X_test_rhog) == Y_test)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'predict test accuracy: '</span>, test_acc)
</pre>
</div>

<pre class="example">
predict test accuracy:  0.1674
</pre>
</div>
</div>

<div id="outline-container-org226e1d8" class="outline-3">
<h3 id="org226e1d8">CS231N 中给出的提出特征的函数</h3>
<div class="outline-text-3" id="text-org226e1d8">
<p>
这里直接使用 <code>diff</code> 计算差分。利用差分计算卷积的速度快很多，根据上面的提示，说如
果寻找到合理的分箱值，验证集准确率可能达到44%。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">from</span> scipy.ndimage <span style="color: #fb4934;">import</span> uniform_filter

<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">extract_features</span>(imgs, feature_fns, verbose=<span style="color: #fabd2f;">False</span>):
    <span style="color: #ada296;">"""</span>
<span style="color: #ada296;">    Given pixel data for images and several feature functions that can operate on</span>
<span style="color: #ada296;">    single images, apply all feature functions to all images, concatenating the</span>
<span style="color: #ada296;">    feature vectors for each image and storing the features for all images in</span>
<span style="color: #ada296;">    a single matrix.</span>

<span style="color: #ada296;">    Inputs:</span>
<span style="color: #ada296;">    - imgs: N x H X W X C array of pixel data for N images.</span>
<span style="color: #ada296;">    - feature_fns: List of k feature functions. The ith feature function should</span>
<span style="color: #ada296;">      take as input an H x W x D array and return a (one-dimensional) array of</span>
<span style="color: #ada296;">      length F_i.</span>
<span style="color: #ada296;">    - verbose: Boolean; if true, print progress.</span>

<span style="color: #ada296;">    Returns:</span>
<span style="color: #ada296;">    An array of shape (N, F_1 + ... + F_k) where each column is the concatenation</span>
<span style="color: #ada296;">    of all features for a single image.</span>
<span style="color: #ada296;">    """</span>
    <span style="color: #83a598;">num_images</span> = imgs.shape[0]
    <span style="color: #fb4934;">if</span> num_images == 0:
        <span style="color: #fb4934;">return</span> np.array([])

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">Use the first image to determine feature dimensions</span>
    <span style="color: #83a598;">feature_dims</span> = []
    <span style="color: #83a598;">first_image_features</span> = []
    <span style="color: #fb4934;">for</span> feature_fn <span style="color: #fb4934;">in</span> feature_fns:
        <span style="color: #83a598;">feats</span> = feature_fn(imgs[0].squeeze())
        <span style="color: #fb4934;">assert</span> <span style="color: #fe8019;">len</span>(feats.shape) == 1, <span style="color: #b8bb26;">'Feature functions must be one-dimensional'</span>
        feature_dims.append(feats.size)
        first_image_features.append(feats)

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">Now that we know the dimensions of the features, we can allocate a single</span>
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">big array to store all features as columns.</span>
    <span style="color: #83a598;">total_feature_dim</span> = <span style="color: #fe8019;">sum</span>(feature_dims)
    <span style="color: #83a598;">imgs_features</span> = np.zeros((num_images, total_feature_dim))
    <span style="color: #83a598;">imgs_features</span>[0] = np.hstack(first_image_features).T

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">Extract features for the rest of the images.</span>
    <span style="color: #fb4934;">for</span> i <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(1, num_images):
        <span style="color: #83a598;">idx</span> = 0
        <span style="color: #fb4934;">for</span> feature_fn, feature_dim <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">zip</span>(feature_fns, feature_dims):
            <span style="color: #83a598;">next_idx</span> = idx + feature_dim
            <span style="color: #83a598;">imgs_features</span>[i, idx:next_idx] = feature_fn(imgs[i].squeeze())
            <span style="color: #83a598;">idx</span> = next_idx
        <span style="color: #fb4934;">if</span> verbose <span style="color: #fb4934;">and</span> i % 1000 == 999:
            <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'Done extracting features for %d / %d images'</span> % (i+1, num_images))

    <span style="color: #fb4934;">return</span> imgs_features


<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">rgb2gray</span>(rgb):
    <span style="color: #ada296;">"""Convert RGB image to grayscale</span>

<span style="color: #ada296;">      Parameters:</span>
<span style="color: #ada296;">        rgb : RGB image</span>

<span style="color: #ada296;">      Returns:</span>
<span style="color: #ada296;">        gray : grayscale image</span>

<span style="color: #ada296;">    """</span>
    <span style="color: #fb4934;">return</span> np.dot(rgb[...,:3], [0.299, 0.587, 0.144])


<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">hog_feature</span>(im):
    <span style="color: #ada296;">"""Compute Histogram of Gradient (HOG) feature for an image</span>

<span style="color: #ada296;">         Modified from skimage.feature.hog</span>
<span style="color: #ada296;">         http://pydoc.net/Python/scikits-image/0.4.2/skimage.feature.hog</span>

<span style="color: #ada296;">       Reference:</span>
<span style="color: #ada296;">         Histograms of Oriented Gradients for Human Detection</span>
<span style="color: #ada296;">         Navneet Dalal and Bill Triggs, CVPR 2005</span>

<span style="color: #ada296;">      Parameters:</span>
<span style="color: #ada296;">        im : an input grayscale or rgb image</span>

<span style="color: #ada296;">      Returns:</span>
<span style="color: #ada296;">        feat: Histogram of Gradient (HOG) feature</span>

<span style="color: #ada296;">    """</span>

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">convert rgb to grayscale if needed</span>
    <span style="color: #fb4934;">if</span> im.ndim == 3:
        <span style="color: #83a598;">image</span> = rgb2gray(im)
    <span style="color: #fb4934;">else</span>:
        <span style="color: #83a598;">image</span> = np.atleast_2d(im)

    <span style="color: #83a598;">sx</span>, <span style="color: #83a598;">sy</span> = image.shape <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">image size</span>
    <span style="color: #83a598;">orientations</span> = 9 <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">number of gradient bins</span>
    <span style="color: #83a598;">cx</span>, <span style="color: #83a598;">cy</span> = (8, 8) <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">pixels per cell</span>

    <span style="color: #83a598;">gx</span> = np.zeros(image.shape)
    <span style="color: #83a598;">gy</span> = np.zeros(image.shape)
    <span style="color: #83a598;">gx</span>[:, :-1] = np.diff(image, n=1, axis=1) <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">compute gradient on x-direction</span>
    <span style="color: #83a598;">gy</span>[:-1, :] = np.diff(image, n=1, axis=0) <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">compute gradient on y-direction</span>
    <span style="color: #83a598;">grad_mag</span> = np.sqrt(gx ** 2 + gy ** 2) <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">gradient magnitude</span>
    <span style="color: #83a598;">grad_ori</span> = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">gradient orientation</span>

    <span style="color: #83a598;">n_cellsx</span> = <span style="color: #fe8019;">int</span>(np.floor(sx / cx))  <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">number of cells in x</span>
    <span style="color: #83a598;">n_cellsy</span> = <span style="color: #fe8019;">int</span>(np.floor(sy / cy))  <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">number of cells in y</span>
    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">compute orientations integral images</span>
    <span style="color: #83a598;">orientation_histogram</span> = np.zeros((n_cellsx, n_cellsy, orientations))
    <span style="color: #fb4934;">for</span> i <span style="color: #fb4934;">in</span> <span style="color: #fe8019;">range</span>(orientations):
        <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">create new integral image for this orientation</span>
        <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">isolate orientations in this range</span>
        <span style="color: #83a598;">temp_ori</span> = np.where(grad_ori &lt; 180 / orientations * (i + 1),
                            grad_ori, 0)
        <span style="color: #83a598;">temp_ori</span> = np.where(grad_ori &gt;= 180 / orientations * i,
                            temp_ori, 0)
        <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">select magnitudes for those orientations</span>
        <span style="color: #83a598;">cond2</span> = temp_ori &gt; 0
        <span style="color: #83a598;">temp_mag</span> = np.where(cond2, grad_mag, 0)
        <span style="color: #83a598;">orientation_histogram</span>[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[<span style="color: #fe8019;">round</span>(cx/2)::cx, <span style="color: #fe8019;">round</span>(cy/2)::cy].T

    <span style="color: #fb4934;">return</span> orientation_histogram.ravel()

<span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">color_histogram_hsv</span>(im, nbin=10, xmin=0, xmax=255, normalized=<span style="color: #fabd2f;">True</span>):
    <span style="color: #ada296;">"""</span>
<span style="color: #ada296;">    Compute color histogram for an image using hue.</span>

<span style="color: #ada296;">    Inputs:</span>
<span style="color: #ada296;">    - im: H x W x C array of pixel data for an RGB image.</span>
<span style="color: #ada296;">    - nbin: Number of histogram bins. (default: 10)</span>
<span style="color: #ada296;">    - xmin: Minimum pixel value (default: 0)</span>
<span style="color: #ada296;">    - xmax: Maximum pixel value (default: 255)</span>
<span style="color: #ada296;">    - normalized: Whether to normalize the histogram (default: True)</span>

<span style="color: #ada296;">    Returns:</span>
<span style="color: #ada296;">      1D vector of length nbin giving the color histogram over the hue of the</span>
<span style="color: #ada296;">      input image.</span>
<span style="color: #ada296;">    """</span>
    <span style="color: #83a598;">ndim</span> = im.ndim
    <span style="color: #83a598;">bins</span> = np.linspace(xmin, xmax, nbin+1)
    <span style="color: #83a598;">hsv</span> = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax
    <span style="color: #83a598;">imhist</span>, <span style="color: #83a598;">bin_edges</span> = np.histogram(hsv[:,:,0], bins=bins, density=normalized)
    <span style="color: #83a598;">imhist</span> = imhist * np.diff(bin_edges)

    <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">return histogram</span>
    <span style="color: #fb4934;">return</span> imhist
</pre>
</div>

<p>
寻找最优的分箱值。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">def</span> <span style="color: #8ec07c;">hog_and_svm</span>(lrs, regs, bins=9, num_iters=1500):
    <span style="color: #83a598;">res</span> = {}

    <span style="color: #83a598;">num_color_bins</span> = bins       <span style="color: #7c6f64;"># </span><span style="color: #7c6f64;">used the features functions</span>
    <span style="color: #83a598;">feature_fns</span> = [hog_feature, <span style="color: #fb4934;">lambda</span> img: color_histogram_hsv(img, nbin=num_color_bins)]
    <span style="color: #83a598;">rt</span> = extract_features(X_train.transpose(0, 2, 3, 1), feature_fns, verbose=<span style="color: #fabd2f;">True</span>)
    <span style="color: #83a598;">rv</span> = extract_features(X_vali.transpose(0, 2, 3, 1), feature_fns, verbose=<span style="color: #fabd2f;">True</span>)
    <span style="color: #83a598;">rte</span> = extract_features(X_test.transpose(0, 2, 3, 1), feature_fns, verbose=<span style="color: #fabd2f;">True</span>)

    <span style="color: #83a598;">best_val_acc</span> = -1

    <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'data preprocess end, train model begin...'</span>)
    <span style="color: #83a598;">svm</span> = LinerSVM()
    <span style="color: #fb4934;">for</span> lr <span style="color: #fb4934;">in</span> lrs:
        <span style="color: #fb4934;">for</span> reg <span style="color: #fb4934;">in</span> regs:
            <span style="color: #83a598;">loss_history</span> = svm.train(rt, Y_train, class_num=10, learning_rate=lr,
                                     regularization=reg, num_iters=num_iters, verbose=<span style="color: #fabd2f;">True</span>)
            <span style="color: #83a598;">res</span>[(lr, reg)] = loss_history

            <span style="color: #83a598;">val_acc</span> = np.mean(svm.predict(rv) == Y_vali)
            <span style="color: #fb4934;">if</span> val_acc &gt; best_val_acc:
                <span style="color: #83a598;">best_val_acc</span> = val_acc
                <span style="color: #83a598;">res</span>[<span style="color: #b8bb26;">'best_val_acc'</span>] = val_acc
                <span style="color: #83a598;">res</span>[<span style="color: #b8bb26;">'best_test_acc'</span>] = np.mean(svm.predict(rte) == Y_test)
                <span style="color: #83a598;">res</span>[<span style="color: #b8bb26;">'best_parameters'</span>] = (lr, reg, num_iters)
    <span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'end'</span>)
    <span style="color: #fb4934;">return</span> res

<span style="color: #83a598;">best_bins_resutls</span> = <span style="color: #fabd2f;">None</span>
<span style="color: #83a598;">best_bin</span> = <span style="color: #fabd2f;">None</span>
<span style="color: #83a598;">best_val_acc</span> = -1
<span style="color: #fb4934;">for</span> b <span style="color: #fb4934;">in</span> [10]:
    <span style="color: #83a598;">r</span> = hog_and_svm(learning_rates, regularization_lambdas, b)
    <span style="color: #fb4934;">if</span> r[<span style="color: #b8bb26;">'best_val_acc'</span>] &gt; best_val_acc:
        <span style="color: #83a598;">best_val_acc</span> = r[<span style="color: #b8bb26;">'best_val_acc'</span>]
        <span style="color: #83a598;">best_bins_results</span> = r
        <span style="color: #83a598;">best_bin</span> = b
</pre>
</div>

<p>
还是没有达到提示中44%验证集准确率。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #fb4934;">print</span>(<span style="color: #b8bb26;">'best_bin: %s, best_val_acc: %s, best_test_acc: %s'</span> %(best_bin, best_val_acc, best_bins_results[<span style="color: #b8bb26;">'best_test_acc'</span>]))
</pre>
</div>

<pre class="example">
best_bin: 10, best_val_acc: 0.235, best_test_acc: 0.2227
</pre>
</div>
</div>
<div id="outline-container-org10c22aa" class="outline-3">
<h3 id="org10c22aa">conclusion</h3>
<div class="outline-text-3" id="text-org10c22aa">
<p>
从最后的结果来看，经过提取特征的分类效果并没有单纯使用 SVM 的准确率高。但从HOG的
思路来思考：一张图片中，人们所能观察到的轮廓往往是轮廓旁边由两种色差比较大的元素
组成。就像拿一支黑色签字笔在白纸上画，你能清晰得看出黑色的轮廓。至于这种色差的程
度是大小该怎样量化？
</p>

<p>
我们知道图片在电脑上显示出来的原因就在于，其由很多的小方格组成，每一个方格中含有
RGB 三个值，用来表示当前方格展示的颜色。那么色差就可以通过两个邻近的方格相减进行
计算。但按照相减的方法只能计算x轴方向与y轴方向的色差，而从图片中，一个居中的元素
方格不可能仅仅具有这两个方向的色差。在二维的图形中，我们知道斜率方向是\(tan(degree) =
\frac{\delta y}{\delta x}\)。那么\(degree = arctan(\frac{ \delta y}{ \delta
x})\)表示的就是色差的方向。而梯度的大小可以通过勾股定理计算出来。
</p>

<p>
对于x轴与y轴来说，梯度大小就是斜率大小，就直接等于一阶差分的结果。利用差分就直接
避免了卷积采集图片在x与y轴方向的梯度大小
</p>

<p>
噪点，表示为与大部分数据点隔离或者与旁边的数据点趋势不同。噪点的存在，会很影响计
算结果。而通过将各小方格连接成一个方格块，不仅仅可以消除噪点，同时还可以减少相对
应的计算量。分块后，如何衡量每一个方格的方向与长度？这就是需要各个方格进行投票决
定。而所谓的投票就是将所有的可能的方向进行分箱，判断各个小方格的方向，将对应的值
落在匹配的分箱中。最后可以得出一个直方图。而这个直方图就表示着小方块的各方向和大
小。
</p>
</div>
</div>
</div>
<div id="outline-container-org91d43a3" class="outline-2">
<h2 id="org91d43a3">Color histogram</h2>
<div class="outline-text-2" id="text-org91d43a3">
<blockquote>
<p>
represents the color of input images while ignoring the texture.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgdf6ed0c" class="outline-2">
<h2 id="orgdf6ed0c">reference</h2>
<div class="outline-text-2" id="text-orgdf6ed0c">
<ol class="org-ol">
<li><a href="https://www.learnopencv.com/histogram-of-oriented-gradients/">Learn Open CV </a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E6%96%B9%E5%90%91%E6%A2%AF%E5%BA%A6%E7%9B%B4%E6%96%B9%E5%9B%BE">HogWiki</a></li>
<li><a href="http://cs231n.github.io/convolutional-networks/">CS231N</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html">Lil'Log</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS --> 
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
