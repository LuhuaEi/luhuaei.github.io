<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-10-29 Tue 09:54 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>使用梯度下降求解线性回归问题</title>
<meta name="generator" content="Org mode">
<meta name="author" content="luhuaei">

<meta name="google-site-verification" content="dVWCUwH8eYXavYgAUJtgmzwlXVIcYZeyvlUolZQVb2E" />
<link rel="stylesheet" type="text/css" href="/assets/css/style.css"/>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div class="content-wrapper container">
   <div class="row"> <div class="col"> </div>   <div class="col-sm-6 col-md-8">
<div id="preamble" class="status">

<div class="">
    <a href="/"> Luhua.ei </a>
</div>
<ul class="">
  <li><a href="/about.html"> About Me </a> </li>
  <li><a href="https://github.com/luhuaei"> Github </a> </li>
  <li><a href="/archive.html"> Posts </a> </li>
</ul>
  <hr>
</div>
<div id="content">
<header>
<h1 class="title">使用梯度下降求解线性回归问题</h1>
</header><p>
<b>梯度下降</b> 为一种优化方法，其原理在于：通过对损失函数(凸函数)进行求导，寻找极小
值所在的方向，沿着该方向前进，直到其达到极小值(可能是全局极小值、也有可能为局部
极小值)。
</p>

<p>
在一个凸函数中，假设其仅有一个极小值点，当某点位于极小值点左边时，模型需要朝右边
前进才是正确的方向，当位于右边时，模型需要往左边前进，才可能找到极小值点。
</p>

<p>
在数学上， <b>梯度</b> 表示成一个多变量函数的全导数(即所有可能的偏导数的组合)。具有一
个参数的函数，在二维空间上表示成一条线，而具有两个参数的函数，在三维空间上表示成
一个平面，而梯度下降就是要找到其中的极小值，对于一个凸曲线来说，极小值处于导数为
0的地方，而对于一个凸面来说，极小值就是处于所有曲线的极小值交点处。
</p>

<div id="outline-container-org7706c2b" class="outline-2">
<h2 id="org7706c2b">优化器</h2>
<div class="outline-text-2" id="text-org7706c2b">
<p>
梯度下降在寻找最小值的过程中，就是一个优化过程。而关于 <b>优化器</b>: 优化器用于根据
损失函数寻找最优的权重值，使得损失函数值最小。我们不可以寻找到最好的权重值在一开
始，而是要通过交互改进的方法进行寻找，即先设定一个随机的权重值，通过每一次对权重
进行修正。
</p>
</div>
<div id="outline-container-org751681b" class="outline-3">
<h3 id="org751681b">随机搜索</h3>
<div class="outline-text-3" id="text-org751681b">
<p>
先随机生成一个权重，对给予的权重值求值损失函数，那当前的损失函数与目前最优的损失
函数相比，如果更优，则保留该权重，否则重复生成一个随机新的权重值，重复比较。
</p>
</div>
</div>
<div id="outline-container-org174f54a" class="outline-3">
<h3 id="org174f54a">随机局部搜索</h3>
<div class="outline-text-3" id="text-org174f54a">
<p>
先随机生成一个权重，每一次都是通过微调权重，如果损失函数更小，则选择为当前最优的
权重，否则，继续对当前的权重进行微调。微调可以为那当前的权重值W，加上一个aw，其
中a代表微调的系数(步伐大小)，w代表一个新的随机权重值。
</p>

<p>
这个方法主要的要点在于，如果随机生成的w权重值与降低损失的方向相同，则更新当前的
权重，如果不同，则忽略。
</p>
</div>
</div>
<div id="outline-container-org408e985" class="outline-3">
<h3 id="org408e985">跟随梯度搜索</h3>
<div class="outline-text-3" id="text-org408e985">
<p>
而跟随梯度搜索为通过梯度可以知道最小损失所在的方向，因此，不需要寻找方向，只需要
更新权重值，令损失达到最小。
</p>

<p>
实现一个梯度下降函数。梯度是一个矢量，具有大小与方向两个特征。梯度始终会指向损失
函数增长最大的方向(通过寻找损失函数中的最大绝对值)而梯度下降就是沿着负梯度的方向
前进(以便尽快降低损失函数)。而下一个前进的点就是当前的点加上梯度的大小，依次重复，
直到极小值(可能为局部极小值，也有可能为全局极小值)。
</p>

<p>
在数学上，梯度表示成一个多变量函数的全导数(即所有可能的偏导数的组合)。具有一个参
数的函数，在二维空间上表示成一条线，而具有两个参数的函数，在三维空间上表示成一
个平面，而梯度下降就是要找到其中的极小值，对于一个凸曲线来说，极小值处于导数为0
的地方，而对于一个凸面来说，极小值就是处于所有曲线的极小值交点处。
</p>

<p>
就好比一个人处于一个盆地的上方，先到达盆地的最低点，对与整个盆地中，除了极小值点
外，其他所有的点所对应的偏导数都是大于0的，即指向损失函数增大的方向，而梯度下降
就是沿着负梯度下降。
</p>
</div>
<div id="outline-container-org6b1acdf" class="outline-4">
<h4 id="org6b1acdf">数值法梯度(numerical gradient)</h4>
<div class="outline-text-4" id="text-org6b1acdf">
<p>
特点为：速度慢，但是容易操作。用泰勒表达式可以验证用一个h的误差项比2h的误差项大。
所以推荐使用2h求导。\(g = \frac{f(x + h) - f(x - h)}{2h}\)
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">def</span> <span style="color: #268bd2;">eval_gradient_numerical</span>(f, x, verbose=<span style="color: #d33682;">True</span>, h=0.00001):
      <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">      Inputs:</span>
<span style="color: #35a69c;">      ------------------------------------------------------------</span>
<span style="color: #35a69c;">      - f: (accept one paramter) loss function.</span>
<span style="color: #35a69c;">      - x: (data set) The point to evaluate the gardient at</span>
<span style="color: #35a69c;">      Outputs:</span>
<span style="color: #35a69c;">      ------------------------------------------------------------</span>
<span style="color: #35a69c;">      - grad: (shape same with x) The gradient on each point of x.</span>
<span style="color: #35a69c;">      '''</span>
      <span style="color: #405A61;"># </span><span style="color: #405A61;">&#29983;&#25104;&#19968;&#20010;&#19982;&#36755;&#20837;x&#30456;&#21516;&#32500;&#24230;&#30340;0&#25968;&#32452;</span>
      <span style="color: #6c71c4;">grad</span> = np.zeros_like(x)
      <span style="color: #405A61;"># </span><span style="color: #405A61;">&#22810;&#32500;&#36845;&#20195;&#65292;&#36825;&#37324;&#23545;x&#30340;&#27599;&#19968;&#20010;&#32500;&#24230;&#37117;&#36827;&#34892;&#36845;&#20195;</span>
      <span style="color: #405A61;"># </span><span style="color: #405A61;">&#20808;&#23545;&#26368;&#21518;&#30340;&#32500;&#24230;&#36827;&#34892;&#36845;&#20195;&#65292;&#24403;&#23436;&#25104;&#21518;&#65292;&#20877;&#23545;&#21069;&#19968;&#20010;&#32500;&#24230;&#36827;&#34892;&#36845;&#20195;</span>
      <span style="color: #405A61;"># </span><span style="color: #405A61;">&#36845;&#20195;&#23436;&#25104;&#21518;&#65292;&#20351;&#29992;finished&#21028;&#26029;</span>
      <span style="color: #6c71c4;">it</span> = np.nditer(x, flags=[<span style="color: #2aa198;">'multi_index'</span>], op_flags=[<span style="color: #2aa198;">'readwrite'</span>])
      <span style="color: #859900;">while</span> <span style="color: #859900;">not</span> it.finished:
          <span style="color: #6c71c4;">ix</span> = it.multi_index
          <span style="color: #6c71c4;">old_val</span> = x[ix]

          <span style="color: #6c71c4;">x</span>[ix] = old_val + h
          <span style="color: #6c71c4;">fxph</span> = f(x)

          <span style="color: #6c71c4;">x</span>[ix] = old_val - h
          <span style="color: #6c71c4;">fxmh</span> = f(x)

          <span style="color: #6c71c4;">x</span>[ix] = old_val

          <span style="color: #405A61;"># </span><span style="color: #405A61;">&#35745;&#31639;&#26012;&#29575;&#65292;&#36825;&#37324;&#20351;&#29992;&#26012;&#29575;&#30340;&#21407;&#22987;&#23450;&#20041;&#35745;&#31639;&#65292;&#36890;&#36807;&#24494;&#23567;&#30340;&#38754;&#31215;&#19982;h&#23485;&#24230;&#12290;</span>
          <span style="color: #6c71c4;">grad</span>[ix] = (fxph - fxmh) / (2 * h)

          <span style="color: #859900;">if</span> verbose:
              <span style="color: #859900;">print</span>(ix, grad[ix])
          it.iternext()
      <span style="color: #859900;">return</span> grad

<span style="color: #859900;">def</span> <span style="color: #268bd2;">eval_gradient_array</span>(f, x, df, h=1e-5):
    <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">    Inputs:</span>
<span style="color: #35a69c;">    ------------------------------------------------------------</span>
<span style="color: #35a69c;">    - f: loss function</span>
<span style="color: #35a69c;">    - x: input data set</span>
<span style="color: #35a69c;">    - df: the loss function output gradient array</span>

<span style="color: #35a69c;">    Outputs:</span>
<span style="color: #35a69c;">    ------------------------------------------------------------</span>
<span style="color: #35a69c;">    - grad: the grad is the gradient array.</span>
<span style="color: #35a69c;">    '''</span>
    <span style="color: #6c71c4;">grad</span> = np.zeros_like(x)
    <span style="color: #6c71c4;">it</span> = np.nditer(x, flags=[<span style="color: #2aa198;">'multi_index'</span>], op_flags=[<span style="color: #2aa198;">'readwrite'</span>])
    <span style="color: #859900;">while</span> <span style="color: #859900;">not</span> it.finished:
        <span style="color: #6c71c4;">ix</span> = it.multi_index
        <span style="color: #6c71c4;">old_val</span> = x[ix]

        <span style="color: #6c71c4;">x</span>[ix] = old_val + h
        <span style="color: #6c71c4;">pos</span> = f(x).copy()

        <span style="color: #6c71c4;">x</span>[ix] = old_val - h
        <span style="color: #6c71c4;">neg</span> = f(x).copy()

        <span style="color: #6c71c4;">x</span>[ix] = old_val
        <span style="color: #6c71c4;">grad</span>[ix] = np.<span style="color: #268bd2;">sum</span>((pos - neg) * df) / (2 * h)
        it.iternext()
    <span style="color: #859900;">return</span> grad
</pre>
</div>
</div>
</div>
<div id="outline-container-org0d4692e" class="outline-4">
<h4 id="org0d4692e">解析法梯度(analytic gradient)</h4>
<div class="outline-text-4" id="text-org0d4692e">
<p>
特点为：数据快，但需要操作精准(易于出错)。利用微积分的原理求解梯度，先把表达式用
积分计算出来，再在代码中实现(求导数)。在现实中，一般采用分析梯度，然后对结果进行检验，即
与数字梯度进行比较。可能一个函数很难或者无法计算导数。
</p>
</div>
</div>
<div id="outline-container-org80c6f02" class="outline-4">
<h4 id="org80c6f02">反向传播梯度(backward pass)</h4>
<div class="outline-text-4" id="text-org80c6f02">
<p>
由于梯度其实就是斜率，也就是函数的导数，通过解析法可以直接求出最后的值，而反向传
播是一步一步向后求导数，只要不是到最后一步，即只要不是到达输入数据层，都被视为一
个函数，再通过链式法则求各个节点的导数。
</p>

<p>
在反向传播过程中，最常见的节点类型具有加、乘、取最大值。当遇到 <b>加号</b> 时，无论前
向传播中是什么值，而在方向传播过程中，该节点输出值保持与输入值不变；当遇到 <b>乘
号</b> 时，如 <code>x*y</code> 对x求导就是y，对y求导就是x；当遇到 <b>最大值符号</b> 时，最大值符号
函数可以理解成一个分段函数 <code>max(x, y)</code> 当 <code>x &gt; y</code> 时，就是对x进行求导，否则就对y
求导，所以需要考虑该节点的前向传播中所有的输入值。
</p>
</div>
</div>
<div id="outline-container-org9adb951" class="outline-4">
<h4 id="org9adb951">梯度审核</h4>
<div class="outline-text-4" id="text-org9adb951">
<p>
梯度检验：使用相对误差来比较两个梯度的差异，这是因为如果使用绝对误差，0.00001在目
标值0.00001和1中是大小程度不相同的。几条检验规则：
</p>
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">relative error</th>
<th scope="col" class="org-left">situation</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&gt;1e-2</td>
<td class="org-left">梯度错误</td>
</tr>

<tr>
<td class="org-left">(1e-2, 1e-4)</td>
<td class="org-left">怀疑(很有可能某个计算出错)</td>
</tr>

<tr>
<td class="org-left">&lt;1e-4</td>
<td class="org-left">对于一些复杂的目标函数可能很好，但对于tanh、softmax这些来说，还是太高</td>
</tr>

<tr>
<td class="org-left">&lt;1e-7</td>
<td class="org-left">表现良好</td>
</tr>
</tbody>
</table>
<p>
需要注意的是，在计算梯度中，使用双精度类型将会更好。而Kinks是导致梯度验证不准确
的一个因素。Kinks与目标函数的不可微分有关、kinks可以通过使用更少的数据集进行验证，
更小的数据集同样令的程序更加高效。
</p>
</div>
</div>
<div id="outline-container-org1ac6bec" class="outline-4">
<h4 id="org1ac6bec">权重更新</h4>
<div class="outline-text-4" id="text-org1ac6bec">
<p>
当利用梯度下降的方法求的每一个位置的梯度后，利用这些梯度对已有的权重进行更新，这
是一个学习的过程，不同的学习速率具有不同大小的前进步伐。 <code>W_new = W -
learning_rate * grad</code> 注意到这里对权重更新是那当前的权重减去对应的梯度，这是因为
斜率的正负表示函数的单调性，如果斜率为正，那随着自变量的增大，因变量也会增大；如
果斜率为负，那随着自变量的增大，因变量减少；而在优化过程中，权重值(W)作为自变量，
损失值作为因变量。而优化器的目的在于是损失达到最小(达到极小值)，故沿着负梯度的方
向前进。
</p>

<p>
梯度只是告诉正确的前进方向，而没有告知前进的步长，步长太短，计算量大，速度慢；步
伐太长，容易超过最低点，甚至还会造成不收敛的情况发生(处于极值点附近动荡)。
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0132df4" class="outline-2">
<h2 id="org0132df4">反向传播</h2>
<div class="outline-text-2" id="text-org0132df4">
<p>
方向传播的主要原理在于使用链式法则。通过前向传播的步骤，一步一步反推。
</p>
</div>
<div id="outline-container-org9cf20c9" class="outline-3">
<h3 id="org9cf20c9">加载数据</h3>
<div class="outline-text-3" id="text-org9cf20c9">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np
<span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt
<span style="color: #859900;">from</span> matplotlib.animation <span style="color: #859900;">import</span> FuncAnimation

plt.style.use(<span style="color: #2aa198;">'ggplot'</span>)
<span style="color: #6c71c4;">DATA</span> = np.array(np.genfromtxt(<span style="color: #2aa198;">"data/gd-line-regression.csv"</span>, delimiter=<span style="color: #2aa198;">','</span>))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf7a2530" class="outline-3">
<h3 id="orgf7a2530">数据预览</h3>
<div class="outline-text-3" id="text-orgf7a2530">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">x</span> = DATA[:, 0]
<span style="color: #6c71c4;">y</span> = DATA[:, 1]
plt.figure(figsize=(9.0, 6.0))
plt.plot(x, y, <span style="color: #2aa198;">'o'</span>)
plt.xlabel(<span style="color: #2aa198;">'x'</span>)
plt.ylabel(<span style="color: #2aa198;">'y'</span>)
plt.tight_layout(pad=0.0)
</pre>
</div>


<figure>
<img src="./images/use-gradient-descent-for-line-regression-945387.png" alt="use-gradient-descent-for-line-regression-945387.png">

</figure>
</div>
</div>
<div id="outline-container-orgefe8be6" class="outline-3">
<h3 id="orgefe8be6">梯度下降函数</h3>
<div class="outline-text-3" id="text-orgefe8be6">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #859900;">class</span> <span style="color: #b58900;">LinearRegression</span>():
    <span style="color: #859900;">def</span> <span style="color: #268bd2;">__init__</span>(<span style="color: #859900;">self</span>):
        <span style="color: #859900;">self</span>.weights_list = []

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">model</span>(<span style="color: #859900;">self</span>, x_train, y_train):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - x_train: (N, D)</span>
<span style="color: #35a69c;">        - y_train: (N, 1)</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #859900;">self</span>.x_train = x_train
        <span style="color: #859900;">self</span>.y_train = y_train

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">train</span>(<span style="color: #859900;">self</span>, weights, learning_rate=0.0001, iter_count=1000):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - weights: (D, 1)</span>
<span style="color: #35a69c;">        - learning_rate: (float)</span>
<span style="color: #35a69c;">        - iter_count: (integer)</span>
<span style="color: #35a69c;">        Outputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        update the weights on self weights.</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #859900;">for</span> i <span style="color: #859900;">in</span> <span style="color: #268bd2;">range</span>(0, iter_count):
            <span style="color: #6c71c4;">gw</span> = <span style="color: #859900;">self</span>.gradient_descent(weights)
            <span style="color: #6c71c4;">weights</span> -= learning_rate * gw
            <span style="color: #405A61;"># </span><span style="color: #405A61;">if don't copy will lead all values will the last update value.</span>
            <span style="color: #859900;">self</span>.weights_list.append(weights.copy())
        <span style="color: #859900;">self</span>.weights = weights


    <span style="color: #859900;">def</span> <span style="color: #268bd2;">gradient_descent</span>(<span style="color: #859900;">self</span>, w):
        <span style="color: #35a69c;">'''</span>
<span style="color: #35a69c;">        use forward and backward pass computer the gradient</span>

<span style="color: #35a69c;">        Inputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - w: (D, 1) weights</span>
<span style="color: #35a69c;">        Outputs:</span>
<span style="color: #35a69c;">        ------------------------------------------------------------</span>
<span style="color: #35a69c;">        - dw: (D, 1) gradient weights</span>
<span style="color: #35a69c;">        '''</span>
        <span style="color: #405A61;"># </span><span style="color: #405A61;">forward pass</span>
        <span style="color: #6c71c4;">w_mul_x</span> = <span style="color: #859900;">self</span>.x_train.dot(w)                           <span style="color: #405A61;"># </span><span style="color: #405A61;">1 (N, 1)</span>
        <span style="color: #6c71c4;">w_mul_x_sub_y</span> = w_mul_x - <span style="color: #859900;">self</span>.y_train                  <span style="color: #405A61;"># </span><span style="color: #405A61;">2 (N, 1)</span>
        <span style="color: #6c71c4;">w_mul_x_sub_y_square</span> = np.square(w_mul_x_sub_y)         <span style="color: #405A61;"># </span><span style="color: #405A61;">3 (N, 1)</span>
        <span style="color: #6c71c4;">sum_w_mul_x_sub_y_square</span> = np.<span style="color: #268bd2;">sum</span>(w_mul_x_sub_y_square) <span style="color: #405A61;"># </span><span style="color: #405A61;">4 (1, 1)</span>
        <span style="color: #6c71c4;">mse</span> = sum_w_mul_x_sub_y_square / <span style="color: #859900;">self</span>.x_train.shape[0]  <span style="color: #405A61;"># </span><span style="color: #405A61;">5 (1, 1)</span>

        <span style="color: #405A61;"># </span><span style="color: #405A61;">backward pass</span>
        <span style="color: #6c71c4;">dsum_w_mul_x_sub_y_square</span> = 1 / <span style="color: #859900;">self</span>.x_train.shape[0] <span style="color: #405A61;"># </span><span style="color: #405A61;">5</span>
        <span style="color: #405A61;"># </span><span style="color: #405A61;">4 equal 1 * dsum_w_mul_x_sub_y_square * self.x_train.shape[0]</span>
        <span style="color: #6c71c4;">dw_mul_x_sub_y_square</span> = 1
        <span style="color: #6c71c4;">dw_mul_x_sub_y</span> = 2 * w_mul_x_sub_y * dw_mul_x_sub_y_square <span style="color: #405A61;"># </span><span style="color: #405A61;">3 (N, 1)</span>
        <span style="color: #6c71c4;">dw_mul_x</span> = dw_mul_x_sub_y                                  <span style="color: #405A61;"># </span><span style="color: #405A61;">2 (N, 1)</span>
        <span style="color: #6c71c4;">dw</span> = <span style="color: #859900;">self</span>.x_train.T.dot(dw_mul_x)                          <span style="color: #405A61;"># </span><span style="color: #405A61;">1 (D, 1)</span>
        <span style="color: #859900;">return</span> dw

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">MSE</span>(<span style="color: #859900;">self</span>):
        <span style="color: #35a69c;">'''computer the mean square error'''</span>
        <span style="color: #6c71c4;">y_pred</span> = np.dot(<span style="color: #859900;">self</span>.x_train, <span style="color: #859900;">self</span>.weights)
        <span style="color: #859900;">return</span> np.mean(np.square(<span style="color: #859900;">self</span>.y_train - y_pred))

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">plot</span>(<span style="color: #859900;">self</span>, w):
        plt.figure(figsize=(9.0, 6.0))
        plt.plot(<span style="color: #859900;">self</span>.x_train[:, 1], <span style="color: #859900;">self</span>.y_train, <span style="color: #2aa198;">'bo'</span>)
        plt.plot(<span style="color: #859900;">self</span>.x_train[:, 1], np.dot(<span style="color: #859900;">self</span>.x_train, w), <span style="color: #2aa198;">'r-'</span>)
        plt.xlabel(<span style="color: #2aa198;">'x'</span>)
        plt.ylabel(<span style="color: #2aa198;">'y'</span>)
        plt.tight_layout(pad=0.0)
        plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc8505fb" class="outline-3">
<h3 id="orgc8505fb">求解</h3>
<div class="outline-text-3" id="text-orgc8505fb">
<p>
这里已经将截距项合并到x中以及weights中。
</p>
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #405A61;"># </span><span style="color: #405A61;">&#22312;&#25968;&#25454;&#21069;&#38754;&#28155;&#21152;&#19968;&#21015;&#65292;&#29992;&#26469;&#35745;&#31639;&#25130;&#36317;&#39033;</span>
<span style="color: #6c71c4;">xt</span> = np.c_[np.ones((x.shape[0])), x]
<span style="color: #6c71c4;">yt</span> = y.reshape(y.shape[0], -1)
<span style="color: #405A61;"># </span><span style="color: #405A61;">&#20004;&#20010;&#31995;&#25968;&#65292;&#19968;&#20010;&#25130;&#36317;&#39033;&#65292;&#19968;&#20010;&#31995;&#25968;</span>
<span style="color: #6c71c4;">weights</span> = np.zeros((2, 1))

<span style="color: #6c71c4;">linear</span> = LinearRegression()
linear.model(xt, yt)
linear.train(weights.copy(), learning_rate=0.000001, iter_count=10)
linear.plot(linear.weights)
</pre>
</div>


<figure>
<img src="./images/use-gradient-descent-for-line-regression-134697.png" alt="use-gradient-descent-for-line-regression-134697.png">

</figure>
</div>
</div>
<div id="outline-container-org2f7ad39" class="outline-3">
<h3 id="org2f7ad39">优化过程</h3>
<div class="outline-text-3" id="text-org2f7ad39">
<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #6c71c4;">fig</span>, <span style="color: #6c71c4;">ax</span> = plt.subplots(figsize=(9.0, 6.0))
ax.scatter(x, y)
line, = ax.plot(x, np.dot(xt, weights), <span style="color: #2aa198;">'r-'</span>, lw=3)

<span style="color: #859900;">def</span> <span style="color: #268bd2;">update</span>(i):
    <span style="color: #6c71c4;">y_pred</span> = np.dot(xt, linear.weights_list[i])
    line.set_ydata(y_pred)
    <span style="color: #859900;">return</span> line,

<span style="color: #6c71c4;">anim</span> = FuncAnimation(fig, update, frames=<span style="color: #268bd2;">list</span>(<span style="color: #268bd2;">range</span>(10)), interval=500)
anim.save(<span style="color: #2aa198;">'./images/update-line-for-gradient-descent.gif'</span>, fps=60, writer=<span style="color: #2aa198;">'imagemagick'</span>)
</pre>
</div>


<figure>
<img src="./images/update-line-for-gradient-descent.gif" alt="update-line-for-gradient-descent.gif">

</figure>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer class="footer">
      <!-- Footer Definition -->
   </footer>

  <!-- Google Analytics Js --><!-- Disqua JS -->
</div>

</div>
<div class="col"></div></div>
</div>
</body>
</html>
