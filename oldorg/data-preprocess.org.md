关于预处理的一个陷阱是：预处理中计算的统计量不是用整个训练集计算得出，而是利用分
割后的训练集计算得出，并应用其他数据集的预处理上。比如：通过计算得出训练集的平均
数为M，那验证集、测试集也是利用M进行中心化。

# 中心化

普通的中心化是直接减去平均值。

# 标准化

标准化是将均值变成0,方差为1。$x = \frac{x - EX}{X_{var}}$

# PCA(主成分分析)

PCA
的应用前提是变量之间是相关的。通过正交矩阵将对称的协方差阵或者相关阵通过正交
矩阵转换成对角矩阵。

1.  先中心化
2.  计算协方差矩阵，协方差用来衡量两个变量的总体误差，当两个变量相等时，表示为变
    量的方差，所以在协方差矩阵中，对角线就是变量的方差、以及协方差矩阵是对称的、
    协方差矩阵是半正定矩阵。
3.  对协方差矩阵进行奇异值分解，通过奇异值分解可以得到协方差矩阵的特征值、特征向
    量。特征向量就是正交矩阵中的列向量，而特征值大小代表在对应的方向的长度(特征值
    就是对应方向上的标准方差的平方，即方差)。利用np.linalg.svd()求值奇异值，返回
    (特征向量、特征值、特征向量(横))
4.  在知道特征向量(相当于新的坐标系)的情况下，通过点积将数据映射到新的坐标上。
5.  通过特征向量可以对数据进行降维处理，因为特征向量每一列代表一个坐标轴，所以我
    们可以指定选择特向向量的个数以达到降维的目的。
6.  而根据前面的知识，我们知道特征值的大小代表数据在该方向的宽度，而奇异分解中，
    返回的结果是经过排序的，所以我们可以根据问题选择前几个特征向量，这就是所谓的
    主成分分析。

# 白化变换(whitening transformation、球化)

球化的步骤差不多与主成分步骤相同，前面通过奇异分解获得特征值、特征向量。然后通过
点积将数据旋转到新的坐标系上，而白化就是将新坐标系上的数据除以特征值的平方跟、
在前面我们说过，特征值代表着数据在对应坐标轴上的长度(宽度)，而现在将每一个数据除
以相对应的特征值平方跟，就是将其标准化(压缩到\[-1,
1\])，这就导致整个图形看起来很
像一个球体。需要注意一点是，为了防止特征值中出现0值，可以加上一个微小的常数。

# 权重初始化

### 全0初始化

在经过合理的标准化后，我们认为数据一半是小于0,一半是大于0,那这是假设权重为0将是
一个合理的选择，但是如果将所有的权重都初始化为0,而如果网络中每一个神经元都是相同
的输出，那将会导致参数具有同样的更新。

### 小的随机值

所以我们需要寻找一个接近于0,而又不等于0的数作为权重值，并且这些数是不对称的(各自
不相同)。小的随机值是一个很好的选择。但并不是初始权重值越小越好，一个神经网络层
有一个很小的权重将会导致在反向传播中具有很像的梯度。

### 校准方差

采用随机值，将有一个问题就是方差将会随着神经元的输入的增大而增大(每一次生成的值
都是不同的)。通过校准方差可以将方差标准化为1。这将确保所有的神经元都具有相同的输
出分布和提高网络的收敛概率。这是因为如果确保所有的权重值都同分布，就具有同方差的
性质，

### 初始化偏差

一般使用0作为初始化偏差，但也有使用一个很接近0,但非0的数作为初始偏差。

### 批量数目

# 正则化(Regularzation)

主要用于防止神经网络过拟合(overfiting)，而根据奥卡姆剃刀原理，可以通过降低模型的
复杂度来防止过拟合。也就是说我们不仅仅是要以最小损失为目标，而是要以最小损失和复
杂度为目标。

### L2正则化

$\frac{1}{\lambda}W^2$其中$\lambda$为正则化率，前面乘以$\frac{1}{2}$是用于
消除求梯度带来的2。L2方法对离群值的惩罚很重，而对于接近于0的几乎没有什么惩罚。这
是因为$x^2$是曲线上升的。

### L1正则化

利用L2正则化，并不能令权重为0,而L1表示为$\lambda|W|$，其可以导致权重为0,而当权
重为0时，会使相对应的特征从模型中移除，即无论输入什么值，输出都为0。对于一个稀疏
矩阵来说，其几乎不可能对噪点输入产生反应。L1适用于特征变量的选择，而L2适用于正规
化。

### 最大约束(Max norm constraints)

指定一个绝对的上界，使用设计好的梯度下降对其约束。在实践过程中，一般的流程是，通
过正常的方式更新参数，然后判断权重是否处于给定的范围内。对于某些问题可以获得很好
的结果，但是如果一个模型所给予的学习速率很高，将可能会导致总是"超界"导致这个模
型继续向下推进。

### Dropout

Dropout定义为：一种简单的方式防止过拟合。Dropout中，一个神经元仅仅有p的概率是激
活的，Dropout可以理解成一个简化版的全链接层。

### 前向传播中的噪点

### 偏差正规化

### 每层正规化

# 损失函数

训练目标：最小化(数据损失+复杂度)，通过正则化可以降低模型的复杂度，防止过拟合的
情况发生。对于一个监督学习来说，一个样本的损失值就是渴望值(真实值)与预测值之间的
差距。而整个模型的损失就是模型的平均损失$L = \frac{1}{N}\sum_iL_i$

当分类的类很多时，使用等级Softmax效果可能会更好，
